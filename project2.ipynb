{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{"id":"rVHwWejfRWSd"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n    \n```\n\n```\ndef get_data(args1, *agrs):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{"id":"EI9ivVwbRWSi"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">Class_counts: (very uneven)</font>\n* id: 0 'githeri': 479,\n* id: 1 'ugali': 628,\n* id: 2 'kachumbari': 494,\n* id: 3 'matoke': 483,\n* id: 4 'sukumawiki': 402,\n* id: 5 'bhaji': 632,\n* id: 6 'mandazi': 620,\n* id: 7 'kukuchoma': 173,\n* id: 8 'nyamachoma': 784,\n* id: 9 'pilau': 329,\n* id:10 'chapati': 862,\n* id:11 'masalachips': 438,\n* id:13 'mukimo': 212\n\nName: class, dtype: int64, sum: 6536\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:22.752062Z","iopub.execute_input":"2023-02-22T12:47:22.752471Z","iopub.status.idle":"2023-02-22T12:47:22.759126Z","shell.execute_reply.started":"2023-02-22T12:47:22.752435Z","shell.execute_reply":"2023-02-22T12:47:22.758084Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:22.764589Z","iopub.execute_input":"2023-02-22T12:47:22.765794Z","iopub.status.idle":"2023-02-22T12:47:32.642969Z","shell.execute_reply.started":"2023-02-22T12:47:22.765755Z","shell.execute_reply":"2023-02-22T12:47:32.641664Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /opt/conda/lib/python3.7/site-packages (0.6.12)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.28.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.64.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.13.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2022.9.24)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Import neccesaary libraries\nimport os\nimport time\nimport collections\nfrom dataclasses import dataclass\nimport sys\n\n# third party library\nimport timm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n\n\n# Pytorch related\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import WeightedRandomSampler\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Subset\nfrom torch.optim import SGD  \nfrom torch.optim import Adam\nfrom torch.optim import AdamW\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom torchvision.models import resnet152, resnet18","metadata":{"id":"xlzaxohURWSi","executionInfo":{"status":"ok","timestamp":1672696976022,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-02-22T12:47:32.646734Z","iopub.execute_input":"2023-02-22T12:47:32.647124Z","iopub.status.idle":"2023-02-22T12:47:32.655970Z","shell.execute_reply.started":"2023-02-22T12:47:32.647090Z","shell.execute_reply":"2023-02-22T12:47:32.654942Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)\ntorch.cuda.device_count()\ntorch.cuda.get_device_name()\ntorch.cuda.is_available()\nprint(torch.cuda.get_device_capability())","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.657774Z","iopub.execute_input":"2023-02-22T12:47:32.658341Z","iopub.status.idle":"2023-02-22T12:47:32.673158Z","shell.execute_reply.started":"2023-02-22T12:47:32.658304Z","shell.execute_reply":"2023-02-22T12:47:32.672241Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"1.11.0\n(6, 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# all the Transforms\n# def resize_preprocess():\n#     \"\"\"Compulsory transforms image to same_size and center cropped (not changing to Tensor yet)\"\"\"\n#     resize_preprocess = transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#     ])\n    \n#     return resize_preprocess\n\n\ndef small_image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset holding original database(No need to hold big image)\"\"\"\n    small_image_preprocess = transforms.Compose([\n        transforms.Resize(1),\n        transforms.ToTensor()\n    ])\n    \n    return small_image_preprocess\n\n\n### from here to below will change to Tensor\ndef image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset \"\"\"\n    image_preprocess = transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n        transforms.Resize((224,224)),\n        transforms.ToTensor()\n    ])\n    \n    return image_preprocess\n\n\ndef train_preprocess():\n    \"\"\"resize_preprocess() + couple transformation to improve accuracy in training, ToTensor and Normalization\"\"\"\n    transforms_train = transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n        transforms.Resize((224,224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        # Somehow this doesn't work\n        #transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio = (0.33, 0.33), value= 0, inplace = False), \n        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n        transforms.RandomRotation(degrees=10),\n        transforms.RandomPosterize(bits=2),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)        \n    ])\n    return transforms_train\n    \n    \ndef validation_preprocess():\n    validation_train = transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)\n    ])\n    return validation_train\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.676305Z","iopub.execute_input":"2023-02-22T12:47:32.676828Z","iopub.status.idle":"2023-02-22T12:47:32.687762Z","shell.execute_reply.started":"2023-02-22T12:47:32.676800Z","shell.execute_reply":"2023-02-22T12:47:32.686107Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Datasets Classes\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"create KenyanFood Dataset from annotations_file and images\"\"\"\n    def __init__(self, transform = None):\n        self.img_labels = pd.read_csv(tc.annotation_file)\n        self.img_dir = tc.img_dir\n        # Make list of the classes list will give index (iterable) -> 13 classes\n        self.classes = list(self.img_labels['class'].unique()) \n        # Make dictionary set of the classes \n        self.dict_classes = dict(enumerate(self.img_labels['class'].unique()))\n        self.transform = transform\n        \n        # weigths of each class normalized (total = 1.0)\n        def _inverse_ratio():\n            \"\"\"calculate inverse ratio of number of each classes for weighted sampler usage\"\"\"\n            series = self.img_labels['class'].value_counts()\n            total = series.sum()\n            for index, _ in enumerate(series):\n                series.iloc[index] = series.iloc[index]/total\n            return series\n        \n        # This inner method won't work for inheritance since it is inside __init__()\n        def _calc_sample_weights():\n            class_counts = self.img_labels['class'].value_counts()\n            sample_weights = [1/class_counts[i] for i in self.img_labels['class']]\n            return sample_weights\n        \n        # sample_weights\n        self.samples_weights = [self.img_labels['class'].value_counts()[i] for i in self.img_labels['class']]       \n    \n    # need this method to get original dataset image_labels from child class\n    def _get_img_labels(self):\n        return self.img_labels  \n        \n    def __len__(self):\n        return len(self.img_labels)\n    \n    def __getitem__(self, index):\n        \"\"\"Return (image, target) after resizing and preprocessing \n        iloc[index, 0] will return ids (e.g.,14278962987112149800) of each row\"\"\"\n        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[index, 0])+\".jpg\")     \n        image = Image.open(img_path)\n\n        # label is string so will return index (pytorch cannot make string to tensor)\n        # iloc[index, 1] will return the class_name (e.g., githeri) for id in [index, 1] (e.g., 14278962987112149800)\n        label_index = self.class2index(self.img_labels.iloc[index, 1])  # returns int\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label_index #image: Image or Tensor ,  label_index: int -> Does this needs to be Tensor?\n    \n    \n    def class2index(self, class_name:str)-> int:\n        \"\"\"Returns the index of a given class.\"\"\"\n        return self.classes.index(class_name)\n    \n    \n    def index2class(self, class_index:int)-> str:\n        \"\"\"Returns the class of a given index.\"\"\"\n        return self.classes[class_index]\n\n\nclass KenyanFood13SplitDataset(KenyanFood13Dataset):\n    def __init__(self, transform = None, train= True):\n        # above __init__() will override super class init(), so needs to call super()__init__ for initialization\n        super().__init__(transform = transform)\n        #self.img_labels = None\n#       print(f\"self.img_labels in __init()__ first: {self.img_labels}\\n\") # 6536\n\n        # need this method to get original dataset image_labels\n        image_labels = super()._get_img_labels()\n\n        # Overrides self.img_labels by ratio of split\n        if train:\n            self.img_labels = image_labels.iloc[:int(len(image_labels) * tc.train_split)]\n        else: #for validation\n            self.img_labels = image_labels.iloc[int(len(image_labels) * tc.train_split):]\n        \n        # Override the sample_weights since train database has splitted database, needed for weighted Random Sampler\n        self.samples_weights = [self.img_labels['class'].value_counts()[i] for i in self.img_labels['class']]\n    \nclass KenyanFood13Testset(Dataset):\n    \"\"\"Kenyan food test dataset, contains original KenanFood13Dataset for image2class and class2image methods\"\"\"\n    def __init__(self, transform = image_preprocess_transforms()):\n        self.img_dir = tc.img_dir\n        self.test_labels = pd.read_csv(tc.test_csv_file)\n        self.transform = transform\n        # this is to use class2index() and class2index()\n        self.base_dataset = KenyanFood13Dataset(small_image_preprocess_transforms())\n\n    def __getitem__(self, index):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        \n        # get img path from test_labels. img_dir + id in csv file (per index) + \".jpg\" \n        img = os.path.join(self.img_dir, str(self.test_labels.iloc[index, 0]) + '.jpg')\n        \n        image = Image.open(img)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n    \n    def __len__(self):\n        return len(self.test_labels)","metadata":{"id":"wmEnYe40RWSj","executionInfo":{"status":"ok","timestamp":1672695586248,"user_tz":-540,"elapsed":237,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-02-22T12:47:32.689370Z","iopub.execute_input":"2023-02-22T12:47:32.690086Z","iopub.status.idle":"2023-02-22T12:47:32.710961Z","shell.execute_reply.started":"2023-02-22T12:47:32.690051Z","shell.execute_reply":"2023-02-22T12:47:32.709849Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{"id":"oG7W87aVRWSk"}},{"cell_type":"code","source":"# Settings and configurations\n@dataclass\nclass SystemConfiguration:\n    seed:int = 42\n        \n# Specifiy all the data needed in dataclass named TrainingConfiguration\n@dataclass\nclass TrainingConfiguration:\n    batch_size: int = 13\n    epoch_count: int = 40\n    init_learning_rate: float = 0.0001\n    lr: float =0.00001\n    log_interval: int = 5\n    test_interval: int = 1\n    data_root: str = \"/kaggle/input/opencv-pytorch-dl-course-classification/\"\n    model_dir: str = '/kaggle/working/models/'\n    log_dir: str = '/kaggle/working/logs'\n    submission_csv: str = '/kaggle/working/submission.csv'\n#     submission_csv: str = '/content/submission.csv'\n#     data_root: str = \"/content/\"    \n#     model_dir: str = '/content/models/'\n#     log_dir: str = '/content/logs'\n    annotation_file: str = data_root + \"train.csv\"\n    test_csv_file: str = data_root + \"test.csv\"\n    img_dir: str = data_root + \"images/images/\"\n\n    num_workers: int = 2\n    device: str = 'cpu'\n    train_split: float = 0.8\n    random_seed: int = 42\n    model_name: str = \"resnet152\"\n    res18_layers :tuple = (2,2, 2,2)\n    res34_layers :tuple = (3,4, 6,3)\n    res50_layers :tuple = (3,4, 6,3)\n    res101_layers:tuple = (3,4,23,3)\n    res152_layers:tuple = (3,8,36,3)\n\n    classes: tuple = ('githeri', 'ugali', 'kachumbari', 'matoke', 'sukumawiki', 'bhaji', 'mandazi', 'kukuchoma', 'nyamachoma', 'pilau', 'chapati', 'masalachips', 'mukimo')\n    dataset_len: int = 6536\n    #below is calculated from KenyanFood13 pictures\n    #mean: torch.tensor = torch.tensor([0.5768, 0.4622, 0.3460])\n    #std: torch.tensor = torch.tensor([0.2699, 0.2739, 0.2826])\n    \n    #below mean and standard are for Resnet18 https://pytorch.org/hub/pytorch_vision_resnet/\n    mean: torch.tensor = torch.tensor([0.485, 0.456, 0.406])\n    std: torch.tensor  = torch.tensor([0.229, 0.224, 0.225])\n        \n    tb_writer: SummaryWriter = SummaryWriter(log_dir)\n    criterion: torch.nn.CrossEntropyLoss = torch.nn.CrossEntropyLoss()        \n\ndef setup_system(SystemConfiguration):\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        tc.device = torch.device(\"cuda:0\")\n        torch.backends.cudnn_benchmark_enabled = True\n        torch.backends.cudnn.deterministic = True\n        \n        \ntc = TrainingConfiguration()\nsystem_config = SystemConfiguration()\nsetup_system(system_config)\n\nfor path in [tc.log_dir, tc.model_dir]:\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.712475Z","iopub.execute_input":"2023-02-22T12:47:32.713076Z","iopub.status.idle":"2023-02-22T12:47:32.784933Z","shell.execute_reply.started":"2023-02-22T12:47:32.713035Z","shell.execute_reply":"2023-02-22T12:47:32.782317Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1597/3468594231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingConfiguration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0msystem_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSystemConfiguration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0msetup_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_1597/3468594231.py\u001b[0m in \u001b[0;36msetup_system\u001b[0;34m(SystemConfiguration)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetup_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSystemConfiguration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."],"ename":"RuntimeError","evalue":"CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.","output_type":"error"}]},{"cell_type":"code","source":"sys.setrecursionlimit(10000) ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.786518Z","iopub.status.idle":"2023-02-22T12:47:32.787176Z","shell.execute_reply.started":"2023-02-22T12:47:32.786913Z","shell.execute_reply":"2023-02-22T12:47:32.786938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{"id":"XtEdfRZjRWSl"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{"id":"qqXcmrqbRWSm"}},{"cell_type":"code","source":"def train(model, train_loader, optimizer) -> float:\n    \"\"\"train by model by given train_loader(and its datasets) and optimizer, return average loss and accuracy\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.to(tc.device)\n    model.train()\n    \n    for inputs, labels in tqdm(train_loader):\n        count += len(labels)\n        # move to target device (GPU or CPU)\n        inputs = inputs.to(tc.device)\n        labels = labels.to(tc.device)\n        \n        # prediction\n        outputs = model(inputs)\n        # calculate loss\n        loss = tc.criterion(outputs, labels)\n        # initialize the gradients -> calucalte the gradients -> Update the gradients\n        optimizer.zero_grad()     \n        loss.backward()\n        optimizer.step()\n        \n        # adds up loss pf number of batches\n        total_loss += loss.item()\n        # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n        avg_loss = total_loss / count\n        \n        # get highest predicted value\n        predicted = torch.max(outputs, axis=1)[1]\n        # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n        total_acc  += (predicted == labels).sum().item()\n        avg_acc =  total_acc / count\n        \n    return avg_loss, avg_acc","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.788745Z","iopub.status.idle":"2023-02-22T12:47:32.789510Z","shell.execute_reply.started":"2023-02-22T12:47:32.789241Z","shell.execute_reply":"2023-02-22T12:47:32.789265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, validation_loader):\n    \"\"\"validate by model by given validation_loader(and its datasets) , return average loss\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.to(tc.device)\n    model.eval()\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(validation_loader):\n            count += len(labels)\n            # move to target device (GPU or CPU)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # prediction\n            outputs = model(inputs)\n            # calculate loss\n            loss = tc.criterion(outputs, labels)\n            \n            # adds up loss pf number of batches\n            total_loss += loss.item()\n            # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n            avg_loss = total_loss / count\n            \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            total_acc  += (predicted == labels).sum().item()\n            # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n            avg_acc =  total_acc / count\n        \n            \n    return avg_loss, avg_acc\n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.791037Z","iopub.status.idle":"2023-02-22T12:47:32.791380Z","shell.execute_reply.started":"2023-02-22T12:47:32.791217Z","shell.execute_reply":"2023-02-22T12:47:32.791234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(trained_model, prediction_loader) -> list:\n    \"\"\"prediction with model and test_loader, returns list of prediction result, assume to be used for inference ONLY\"\"\"  \n    predictions = torch.tensor([]).to(tc.device)\n    for inputs in prediction_loader:\n        inputs = inputs.to(tc.device)\n        outputs = trained_model(inputs)\n        # get the predicted result by index number, each prediction will be done by tc.batch_size so predicted is array of batch_size\n        predictions = torch.hstack((predictions, torch.max(outputs, axis=1)[1]))\n        \n    return predictions.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.793404Z","iopub.status.idle":"2023-02-22T12:47:32.793865Z","shell.execute_reply.started":"2023-02-22T12:47:32.793628Z","shell.execute_reply":"2023-02-22T12:47:32.793651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{"id":"0-ysifviRWSm"}},{"cell_type":"code","source":"#implement Resnet from Scratch\n# conv3x3 and conv1x1 as static method since used a lot.\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride=stride, padding=1, bias=False)\n\ndef conv1x1(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.795422Z","iopub.status.idle":"2023-02-22T12:47:32.796547Z","shell.execute_reply.started":"2023-02-22T12:47:32.796250Z","shell.execute_reply":"2023-02-22T12:47:32.796278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    # at the last layer of each block, the channel is x1, for example resnet30 first Bottleneck,\n    # 1x1 conv 64 -> 3x3 conv 64\n    expansion = 1\n\n    def __init__(self, in_channels, base_channels, stride=1):\n        \"\"\"\n        Constructor\n\n        Parameters\n        ----------\n        in_channels : int\n            number of channels for input\n        base_channels : int\n            number of channels used in this basic block\n        stride : int\n            stride used in this basic block\n        \"\"\"\n        super().__init__()\n        self.conv1 = conv3x3(in_channels, base_channels, stride)\n        self.bn1 = nn.BatchNorm2d(base_channels)\n        self.relu = nn.ReLU(inplace=True)  # inplace=True will directory change the value\n        self.conv2 = conv3x3(base_channels, base_channels)\n        self.bn2 = nn.BatchNorm2d(base_channels)\n\n        # First residual block's number of input channel and output channel is different (e.g., conv3_x in res18 has input channel as 64 but out put is 128)\n        # And also the size (56 x 56 -> 28 x 28),\n        if in_channels != base_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                conv1x1(in_channels, base_channels * self.expansion, stride=2),  # stride = 2 due to make the size half\n                nn.BatchNorm2d(base_channels * self.expansion)\n            )\n            print(f\"self.shortcut for in_channels != channels * self.expansion: {self.shortcut}\")\n        else:\n            self.shortcut = nn.Sequential()\n            print(f\"self.shortcut for in_channels == channels * self.expansion: {self.shortcut}\")\n\n        print(\n            f\"##### in_channels:{in_channels}  base_channels:{base_channels} conv1 stride: {stride}, self.shortcut {self.shortcut}\")\n\n    def forward(self, x):\n        \"\"\"\n\n        Parameters\n        ----------\n        self\n        x\n\n        Returns\n        -------\n\n        \"\"\"\n        out = self.conv1(x)\n        print(f\"conv1(x).size(): {out.shape}\")\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += self.shortcut(x)\n        print(f\"self.shortcut(x).shape: {self.shortcut(x).shape}\")\n        print(f\"out += self.shortcut(x): {out.shape}\")\n        out = self.relu(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.798173Z","iopub.status.idle":"2023-02-22T12:47:32.798658Z","shell.execute_reply.started":"2023-02-22T12:47:32.798411Z","shell.execute_reply":"2023-02-22T12:47:32.798436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    # at the last layer of each block, the channel is x4, for example resnet50 first Bottleneck,\n    # 1x1 conv 64 -> 3x3 conv 64 -> 1x1 conv 256\n    expansion = 4\n\n    def __init__(self, in_channels, base_channels, stride=1):\n        \"\"\"\n\n        Parameters\n        ----------\n        in_channels\n        base_channels\n        stride\n        \"\"\"\n        super().__init__()\n        self.conv1 = conv1x1(in_channels, base_channels)\n        self.bn1 = nn.BatchNorm2d(base_channels)\n        self.conv2 = conv3x3(base_channels, base_channels, stride)\n        self.bn2 = nn.BatchNorm2d(base_channels)\n        # third one has 4x more channels as out put\n        self.conv3 = conv1x1(base_channels, base_channels * self.expansion)\n        self.bn3 = nn.BatchNorm2d(base_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n\n        if in_channels != base_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                # need stride as parameter since conv2_x is stride=1, and others are 2\n                conv1x1(in_channels, base_channels * self.expansion, stride),\n                nn.BatchNorm2d(base_channels * self.expansion),\n            )\n            print(f\"self.shortcut for in_channels != channels * self.expansion: {self.shortcut}\")\n        else:\n            self.shortcut = nn.Sequential()\n            print(f\"self.shortcut for in_channels == channels * self.expansion: {self.shortcut}\")\n\n        print(\n            f\"##### in_channels:{in_channels}  base_channels:{base_channels} conv2 stride: {stride}, self.shortcut {self.shortcut}\")\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += self.shortcut(x)\n        print(f\"self.shortcut(x).shape: {self.shortcut(x).shape}\")\n        print(f\"out += self.shortcut(x): {out.shape}\")\n\n        out = self.relu(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.800172Z","iopub.status.idle":"2023-02-22T12:47:32.800623Z","shell.execute_reply.started":"2023-02-22T12:47:32.800360Z","shell.execute_reply":"2023-02-22T12:47:32.800380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Resnet(nn.Module):\n    def __init__(self, block_type, layers: tuple, num_classes=1000):\n        \"\"\"\n\n        Parameters\n        ----------\n        block_type: BasicBlock, Bottleneck\n            Whether this resnet use BasicBlock (Res18/Res34) or Bottleneck(Res50 or higher)\n        layers: tuple\n            contains number of covolution in each layer, for example for Resnet50 (3,4,6,4)\n        num_classes: int\n            number of output classes\n        \"\"\"\n        super().__init__()\n\n        self.in_channels = 64  # initial channel for all type is 64\n\n        # initial conv1_x layer, number of input_channel = 3(R,G,B), number of output_channel = 64\n        # Input image size 224*224*3(RGB), padding = (3,3) to make size 230 * 230 ->  112*112 with stride (2,2)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        # conv2_x layer, input size is (batch_size, 64, 112, 112)\n        self.maxpool = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv2_x = self._create_layer(block_type, 64, layers[0], stride=1)  # for resnet50 layers[0] = 3\n        self.conv3_x = self._create_layer(block_type, 128, layers[1], stride=2)  # for resnet50 layers[1] = 4\n        self.conv4_x = self._create_layer(block_type, 256, layers[2], stride=2)  # for resnet50 layers[2] = 6\n        self.conv5_x = self._create_layer(block_type, 512, layers[3], stride=2)  # for resnet50 layers[3] = 3\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block_type.expansion, num_classes)\n\n    def _create_layer(self, block_type, base_channels, repeated_conv_blocks, stride):\n        \"\"\"\n        create layers, conv2_x, conv3_x, conv4_x and conv5_x in Resnet18, 34, 50, 101 and 152\n\n        Parameters\n        ----------\n        block_type:BasicBlock, Bottleneck\n            Whether this resnet use BasicBlock (Res18/Res34) or Bottleneck(Res50 or higher)\n        base_channels: int\n            for conv2_x: 64, conv3_x:128 , conv4_x = 256, conv5_x 512. These numbers are common across all Resnet type\n        repeated_conv_blocks : int\n            number of convolution blocks repeated, for example res50 conv2_x repeats 3 times and conv4_x repeats 6 times\n        stride: int or tuple(int,int)\n\n        Returns\n        -------\n            nn.Sequential\n        \"\"\"\n        layers = []\n\n        # first Residual basic block (or bottleneck), constructs basic block (or bottleneck) and appends to list\n        layers.append(block_type(self.in_channels, base_channels, stride))\n        # rest of Residual basic block\n        self.in_channels = base_channels * block_type.expansion\n        for _ in range(1, repeated_conv_blocks):\n            layers.append(block_type(self.in_channels, base_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.maxpool(x)\n\n        x = self.conv2_x(x)\n        x = self.conv3_x(x)\n        x = self.conv4_x(x)\n        x = self.conv5_x(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.802914Z","iopub.status.idle":"2023-02-22T12:47:32.803448Z","shell.execute_reply.started":"2023-02-22T12:47:32.803162Z","shell.execute_reply":"2023-02-22T12:47:32.803188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use pretrained one so comment out\n# def resnet18():\n#     return ResNet(BasicBlock, tc.res18_layers)\n# def resnet34():\n#     return ResNet(BasicBlock, tc.res34_layers)\n# def resnet50():\n#     return ResNet(Bottleneck, tc.res50_layers)\n# def resnet101():\n#     return Resnet(Bottleneck, tc.res101_layers)\n# def resnet152():\n#     return Resnet(Bottleneck, tc.re152_layers)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.805396Z","iopub.status.idle":"2023-02-22T12:47:32.805913Z","shell.execute_reply.started":"2023-02-22T12:47:32.805652Z","shell.execute_reply":"2023-02-22T12:47:32.805676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Resnet 18 (or 152) and change last layer output to 13\n# model = resnet152(pretrained = True)\n# # Change the last output to 13\n# fc_in_features = model.fc.in_features\n# model.fc = torch.nn.Linear(fc_in_features, (len(tc.classes)))\n# model = model.to(tc.device)\n","metadata":{"id":"fRNx53rwRWSn","execution":{"iopub.status.busy":"2023-02-22T12:47:32.807286Z","iopub.status.idle":"2023-02-22T12:47:32.807629Z","shell.execute_reply.started":"2023-02-22T12:47:32.807467Z","shell.execute_reply":"2023-02-22T12:47:32.807483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdditionalDenseLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.classifier_layer = nn.Sequential(\n            nn.Linear(2048, 1024),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),   \n            nn.Linear(512, 256),\n            nn.Linear(256, len(tc.classes))\n        )\n    \n    def forward(self, x):\n        x = self.classifier_layer(x)\n        \n        return x","metadata":{"id":"7mYFpjLQRWSn","execution":{"iopub.status.busy":"2023-02-22T12:47:32.810203Z","iopub.status.idle":"2023-02-22T12:47:32.810956Z","shell.execute_reply.started":"2023-02-22T12:47:32.810712Z","shell.execute_reply":"2023-02-22T12:47:32.810736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{"id":"mwFF7LkBRWSn"}},{"cell_type":"code","source":"## util functions\n\ndef get_mean_std(dataset):\n    \"\"\"returns mean and standard deviation of dataset given, since this method will take long time, once calcuated, not been used\"\"\"\n    # calculated mean: tensor([0.5768, 0.4622, 0.3460]), std:tensor([0.2699, 0.2739, 0.2826]):\n    loader = DataLoader(dataset)\n    \n    batch_mean = torch.zeros(3) # tensor([0,0,0])\n    batch_mean_sqrd = torch.zeros(3)\n    \n    for batch_data, _ in loader:\n        batch_mean += batch_data.mean(dim=(0,2,3))\n        batch_mean_sqrd += (batch_data **2).mean(dim=(0,2,3))\n        \n    mean = batch_mean / len(loader)\n    var = (batch_mean_sqrd) / len(loader) - (mean **2)\n    \n    std = var ** .5\n    \n    print(\"mean: {}, std:{}:\".format(mean, std))\n    return mean, std\n\n\ndef eval_loss(data_loader, device, model, criterion):\n    \"\"\"evaluate losses and can use this for visualize\"\"\"\n    for images, labales in data_loader:\n        break\n        \n    inputs = images.to(device)\n    labels = labels.to(device)\n    \n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    \n    return loss\n\ndef evaluate_history(history):\n    import matplotlib.pyplot as plt\n    print(\"initial stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[0,3], history[0,4]))\n    print(\"final stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[-1,3], history[-1,4]))\n    \n    num_epochs = len(history)\n    unit = num_epochs/10\n    \n    # Display learning curve (Loss)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,1], 'b', label='train')\n    plt.plot(history[:,0], history[:,3], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1, unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Loss)')\n    plt.legend()\n    plt.show()\n\n    # Display learning curve (Accuracy)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,2], 'b', label='train')\n    plt.plot(history[:,0], history[:,4], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1,unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Accuracy')\n    plt.legend()\n    plt.show()\n\n\n# model save and load functions\ndef save_model(model, device):\n    if not os.path.exists(tc.model_dir):\n        os.makedirs(tc.model_dir)\n    \n    model_path = os.path.join(tc.model_dir, tc.model_name)\n    \n    if device == \"cuda\":\n        model.to(\"cpu\")\n    \n    #torch.save(model.state_dict(), model_path + \"best_model.pt\")\n    torch.save(model.state_dict(), os.path.join(tc.model_dir, \"best_model.pt\"))\n    \n    if device == \"cuda\":\n        model.to(\"cuda\")\n    \n    return\n\ndef load_model(model):\n    \n    #model_path = os.path.join(tc.model_dir, tc.model_name)\n    model.load_state_dict(torch.load(tc.model_dir + 'best_model.pt'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.812329Z","iopub.status.idle":"2023-02-22T12:47:32.813099Z","shell.execute_reply.started":"2023-02-22T12:47:32.812852Z","shell.execute_reply":"2023-02-22T12:47:32.812877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* id: 0 'githeri': 479,\n* id: 1 'ugali': 628,\n* id: 2 'kachumbari': 494,\n* id: 3 'matoke': 483,\n* id: 4 'sukumawiki': 402,\n* id: 5 'bhaji': 632,\n* id: 6 'mandazi': 620,\n* id: 7 'kukuchoma': 173,\n* id: 8 'nyamachoma': 784,\n* id: 9 'pilau': 329,\n* id:10 'chapati': 862,\n* id:11 'masalachips': 438,\n* id:13 'mukimo': 212","metadata":{}},{"cell_type":"code","source":"#tools to use matplot to visualize data balanced each batch\ndef visualize_batch_data(dataloader, with_visual_chart = True , with_class_string = True) -> dict:\n    \"\"\"visualize what class is in the batch to visually see the equalness of the samples\"\"\"\n    #{0: 'githeri', 1: 'ugali', 2: 'kachumbari', 3: 'matoke', 4: 'sukumawiki', 5: 'bhaji', 6: 'mandazi', 7: 'kukuchoma', 8: 'nyamachoma', 9: 'pilau', 10: 'chapati', 11: 'masalachips', 12: 'mukimo'}\n    # total_number_appeared: Counter({10: 862, 8: 784, 5: 632, 1: 628, 6: 620, 2: 494, 3: 483, 0: 479, 11: 438, 4: 402, 9: 329, 12: 212, 7: 173})\n    total_num_images = len(data_loader.dataset)\n    total_number_appeared = collections.Counter()\n    number_appeared_per_batch = []\n    dict_classes = data_loader.dataset.dict_classes\n    \n    for index, (_, label_index) in enumerate(data_loader): # don't use image\n#         class_ids, class_counts = torch.unique(label_index, return_counts=True) # return in torch.tensor\n        number_appeared = collections.Counter(label_index.tolist())\n        print(f\"batch:{index} number_appeared:{number_appeared}\")\n        \n        # add zero for sample which was not picked, and add that label_index:0 to Counter\n        while len(number_appeared) < len(dict_classes):\n            for i in range(len(number_appeared)):\n                number_appeared.setdefault(i,0)\n            print(f\"batch:{index} number_appeared_after_setdefault:{number_appeared}\")\n        # for total count\n        total_number_appeared += number_appeared\n\n            \n        # sort based on keys (label_index) and cast to dict for visualization\n        dict_number_appeared = dict(sorted(number_appeared.items()))\n        #print(f\"dict_number_appeared:{dict_number_appeared}\")\n        number_appeared_per_batch.append(dict_number_appeared)\n        \n    if with_visual_chart:\n        \"\"\"create bar graph for all batches which class had samples\"\"\"\n        fig, axes = plt.subplots(int(tc.batch_size/4), 4 , figsize = (20, 20))\n        for i in range(int(tc.batch_size/4)):\n            for j in range(4):\n                axes[i][j].bar(number_appeared_per_batch[int(j+(i*4))].keys(), list(number_appeared_per_batch[int(j+(i*4))].values()))\n                if with_class_string:\n                    axes[i][j].set_xticks(list(range(len(dict_classes))))\n                    axes[i][j].set_xticklabels(list(dict_classes.values()))\n                else:\n                    axes[i][j].set_xticks(list(range(len(dict_classes))))\n                    axes[i][j].set_xticklabels(list(dict_classes.keys()))\n\n    \n    # return dict type, key=class name, value=how many times appeared\n    if with_class_string:\n        return {data_loader.dataset.index2class(key):value for key, value in total_number_appeared.items()}\n    \n    # return dict type, key=class_id, value=how many times appeared\n    return dict(total_number_appeared)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:32.814463Z","iopub.status.idle":"2023-02-22T12:47:32.815202Z","shell.execute_reply.started":"2023-02-22T12:47:32.814952Z","shell.execute_reply":"2023-02-22T12:47:32.814977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{"id":"tyZ0aRAaRWSn"}},{"cell_type":"code","source":"# use timm.resnetrs50 as backborn and add couple linear layers\nmodel = timm.create_model('resnetrs50', pretrained=True, num_classes=1000) \n# freeze the backborn and do training again\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.fc = AdditionalDenseLayer()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:50:46.566482Z","iopub.execute_input":"2023-02-22T12:50:46.567530Z","iopub.status.idle":"2023-02-22T12:50:47.211438Z","shell.execute_reply.started":"2023-02-22T12:50:46.567480Z","shell.execute_reply":"2023-02-22T12:50:47.210531Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = KenyanFood13SplitDataset(transform = train_preprocess(),train = True)\nvalidation_dataset = KenyanFood13SplitDataset(transform = validation_preprocess(), train = False)\nsequential_sampler = torch.utils.data.SequentialSampler(train_dataset)\nweight_sampler = WeightedRandomSampler(weights=train_dataset.samples_weights, num_samples=len(train_dataset), replacement=True)\ntrain_loader = DataLoader(train_dataset, sampler = weight_sampler, batch_size = tc.batch_size, shuffle = False)\nvalidation_loader = DataLoader(validation_dataset, batch_size = tc.batch_size, shuffle = False )\n# create optimizer\noptimizer = AdamW(model.parameters(), lr=tc.init_learning_rate)\nprint(f\"length of train_dataset {len(train_dataset.samples_weights)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:47:56.399489Z","iopub.execute_input":"2023-02-22T12:47:56.400127Z","iopub.status.idle":"2023-02-22T12:48:07.184733Z","shell.execute_reply.started":"2023-02-22T12:47:56.400089Z","shell.execute_reply":"2023-02-22T12:48:07.183522Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"length of train_dataset 5228\n","output_type":"stream"}]},{"cell_type":"code","source":"# sample1 = train_loader.dataset.__getitem__(0)[0].shape\n# print(sample1)\n\nfor i in range(10):\n    sample = train_loader.dataset.__getitem__(i)[0].shape\n    print(sample)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:48:13.844728Z","iopub.execute_input":"2023-02-22T12:48:13.845087Z","iopub.status.idle":"2023-02-22T12:48:14.204527Z","shell.execute_reply.started":"2023-02-22T12:48:13.845055Z","shell.execute_reply":"2023-02-22T12:48:14.203462Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"torch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_val_loss = np.Inf\nhistory = np.zeros((0,6))\nfor epoch in range(tc.epoch_count):\n    # time one cycle of train and validate\n    start_time = time.time()\n    train_loss, train_acc = train(model, train_loader, optimizer)\n    val_loss, val_acc = validate(model, validation_loader)\n    elapsed_time = time.time() - start_time\n    \n    # save the weight when the loss gets smaller than last one\n    if val_loss < best_val_loss:\n        save_model(model, tc.device)\n        print(\"WEIGHTS-ARE-SAVED\")\n        best_val_loss = val_loss\n        \n    print(f\"Epoch{epoch+1}/{tc.epoch_count}, loss:{train_loss:.5f}, acc:{train_acc:.5f}, val_loss:{val_loss:.5f}, val_acc:{val_acc:.5f}, elapsed_time:{elapsed_time:.5f}\")\n    # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n    item = np.array([epoch+1, train_loss, train_acc, val_loss, val_acc, elapsed_time])\n    # store history for matplotlib visualization\n    history = np.vstack((history, item))\n\n    # For tensorboard\n    tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_loss, epoch+1)\n    tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_acc, epoch+1)\n    tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:48:14.206660Z","iopub.execute_input":"2023-02-22T12:48:14.207048Z","iopub.status.idle":"2023-02-22T12:50:06.296960Z","shell.execute_reply.started":"2023-02-22T12:48:14.207009Z","shell.execute_reply":"2023-02-22T12:50:06.294664Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/403 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef616268ead94d1abccad1a5d495fe24"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1597/1352448719.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# time one cycle of train and validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_1597/4169725596.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/resnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2422\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m     )\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_history(history)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:50:06.297933Z","iopub.status.idle":"2023-02-22T12:50:06.298411Z","shell.execute_reply.started":"2023-02-22T12:50:06.298152Z","shell.execute_reply":"2023-02-22T12:50:06.298174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unfreeze the backborn and do training again\nfor param in model.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:50:06.300971Z","iopub.status.idle":"2023-02-22T12:50:06.301485Z","shell.execute_reply.started":"2023-02-22T12:50:06.301212Z","shell.execute_reply":"2023-02-22T12:50:06.301236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unfreeze the backborn and do training again\ntrain_dataset = KenyanFood13SplitDataset(transform = train_preprocess(),train = True)\nvalidation_dataset = KenyanFood13SplitDataset(transform = validation_preprocess(), train = False)\nsequential_sampler = torch.utils.data.SequentialSampler(train_dataset)\nweight_sampler = WeightedRandomSampler(weights=train_dataset.samples_weights, num_samples=len(train_dataset), replacement=True)\ntrain_loader = DataLoader(train_dataset, sampler = weight_sampler, batch_size = tc.batch_size, shuffle = False)\nvalidation_loader = DataLoader(validation_dataset, batch_size = tc.batch_size, shuffle = False )\n# create optimizer\noptimizer = AdamW(model.parameters(), lr=tc.lr)\nprint(f\"length of train_dataset {len(train_dataset.samples_weights)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:50:06.303202Z","iopub.status.idle":"2023-02-22T12:50:06.303755Z","shell.execute_reply.started":"2023-02-22T12:50:06.303470Z","shell.execute_reply":"2023-02-22T12:50:06.303496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now do the inference and get submission.csv file\ntest_dataset = KenyanFood13Testset()\ntest_loader = DataLoader(test_dataset, batch_size = tc.batch_size, shuffle = False)\n\n# prediction\npredicted_result = predict(model, test_loader)\n#print(\"predicted_result len:{}\".format(len(predicted_result)))\nclassifications = []\n\n#change predicted index to class_names\nfor index in range(len(predicted_result)):\n    predicted_result_class = test_dataset.base_dataset.index2class(int(predicted_result[index]))\n    classifications.append(predicted_result_class)\n\n# put it to csv file\nclasses = pd.DataFrame(classifications, columns = [\"class\"])\nresult = test_dataset.test_labels.join(classes)\nresult.to_csv(tc.submission_csv, index=False, header = True)","metadata":{"id":"WqFnOpy1RWSn","execution":{"iopub.status.busy":"2023-02-22T12:50:06.305544Z","iopub.status.idle":"2023-02-22T12:50:06.306023Z","shell.execute_reply.started":"2023-02-22T12:50:06.305777Z","shell.execute_reply":"2023-02-22T12:50:06.305800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"f7bNvOcZRWSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dont use anymore... refactored\n# def fit(model, optimizer, train_loader, validation_loader, history):\n#     \"\"\"works like tensorflow fits. epoch_count, optimizer and criterion(e.g., CrossEntorpyLoss) from training config\"\"\"\n    \n#     #from tqdm.notebook import tqdm\n    \n#     base_epochs = len(history)\n    \n#     for epoch in range(base_epochs, tc.epoch_count + base_epochs):\n#         train_loss = 0\n#         train_acc = 0\n#         val_loss = 0\n#         val_acc = 0\n#         best_valid_loss = np.Inf\n        \n#         # training Phase\n#         model.train()\n#         count = 0\n        \n#         t_begin = time.time()\n#         for inputs, labels in train_loader:\n            \n#             count += len(labels)\n#             inputs = inputs.to(tc.device)\n#             labels = labels.to(tc.device)\n            \n#             # initialize the gradients\n#             optimizer.zero_grad()\n#             # predict\n#             outputs = model(inputs)\n#             # calculate loss\n#             loss = tc.criterion(outputs, labels)\n#             train_loss += loss.item()\n            \n#             #calucalte the graidents\n#             loss.backward()\n#             #update parameter\n#             optimizer.step()\n            \n#             # get highest predicted value\n#             predicted = torch.max(outputs, axis=1)[1]\n#             # calculate number of correct answer\n#             train_acc  += (predicted == labels).sum().item()\n            \n#             #calculate the average loss and accuracy in the batch\n#             avg_train_loss = train_loss / count\n#             avg_train_acc = train_acc / count\n            \n#         #evaluation phase\n#         model.eval()\n#         count = 0\n        \n#         for inputs, labels in validation_loader:\n#             count += len(labels)\n#             inputs = inputs.to(tc.device)\n#             labels = labels.to(tc.device)\n            \n#             # calculate the prediction\n#             outputs = model(inputs)\n            \n#             # calculate the loss\n#             loss = tc.criterion(outputs, labels)\n#             val_loss += loss.item()\n            \n#             if val_loss < best_valid_loss:\n#                 torch.save(model.state_dict(), \"best_model.pt\")\n#                 print(\"WEIGHTS-ARE-SAVED\")\n#                 best_valid_loss = val_loss\n                \n#             # get highest predicted value\n#             predicted = torch.max(outputs, axis=1)[1]\n#             # calculate number of correct answer\n#             val_acc += (predicted == labels).sum().item()\n            \n#             #calculate the average loss and accuracy in the batch\n#             avg_val_loss = val_loss / count\n#             avg_val_acc = val_acc / count\n            \n            \n#         elapsed_time = time.time() - t_begin\n#         # print out\n#         print(\"Epoch{}/{},  loss:{:.5f}, acc:{:.5f}, val_loss:{:.5f}, val_acc:{:.5f}, elapsed_time:{:.5f}\".format(\n#             epoch+1,tc.epoch_count+base_epochs, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time\n#         ))\n#         # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n#         item = np.array([epoch+1, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time])\n#         history = np.vstack((history, item))\n        \n#         # For tensorboard\n#         tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_loss, epoch+1)\n#         tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_acc, epoch+1)\n#         tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)\n\n        \n#     return history","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:50:06.307706Z","iopub.status.idle":"2023-02-22T12:50:06.308186Z","shell.execute_reply.started":"2023-02-22T12:50:06.307940Z","shell.execute_reply":"2023-02-22T12:50:06.307963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n\n**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\nFor example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars).","metadata":{"id":"QRbjuol1RWSo"}},{"cell_type":"code","source":"/kaggle/working/logs/events.out.tfevents.1676751468.7b40718cd348.24.0\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:50:06.309857Z","iopub.status.idle":"2023-02-22T12:50:06.310383Z","shell.execute_reply.started":"2023-02-22T12:50:06.310111Z","shell.execute_reply":"2023-02-22T12:50:06.310137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"https://github.com/caq05630","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:50:06.312115Z","iopub.status.idle":"2023-02-22T12:50:06.312600Z","shell.execute_reply.started":"2023-02-22T12:50:06.312340Z","shell.execute_reply":"2023-02-22T12:50:06.312363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"NHuqwjtrRWSo"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{"id":"zP0rAnxrRWSo"}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/yasuokada/project2/edit/run/119205728","metadata":{}}]}