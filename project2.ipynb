{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{"id":"rVHwWejfRWSd"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n    \n```\n\n```\ndef get_data(args1, *agrs):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{"id":"EI9ivVwbRWSi"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**class_counts: (very uneven)**\n1. chapati        862\n1. nyamachoma     784\n1. bhaji          632\n1. ugali          628\n1. mandazi        620\n1. kachumbari     494\n1. matoke         483\n1. githeri        479\n1. masalachips    438\n1. sukumawiki     402\n1. pilau          329\n1. mukimo         212\n1. kukuchoma      173\n\nName: class, dtype: int64, sum: 6536\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:50.702679Z","iopub.execute_input":"2023-01-31T06:34:50.703060Z","iopub.status.idle":"2023-01-31T06:34:50.708883Z","shell.execute_reply.started":"2023-01-31T06:34:50.703029Z","shell.execute_reply":"2023-01-31T06:34:50.707820Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Import neccesaary libraries\nimport os\nimport time\nimport collections\nfrom dataclasses import dataclass\n\n# third party library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n\n# Pytorch related\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import WeightedRandomSampler\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Subset\nfrom torch.optim import SGD  \nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom torchvision.models import resnet152, resnet18","metadata":{"id":"xlzaxohURWSi","executionInfo":{"status":"ok","timestamp":1672696976022,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-01-31T06:34:50.724930Z","iopub.execute_input":"2023-01-31T06:34:50.725800Z","iopub.status.idle":"2023-01-31T06:34:50.733274Z","shell.execute_reply.started":"2023-01-31T06:34:50.725767Z","shell.execute_reply":"2023-01-31T06:34:50.732255Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"# all the Transforms\n# def resize_preprocess():\n#     \"\"\"Compulsory transforms image to same_size and center cropped (not changing to Tensor yet)\"\"\"\n#     resize_preprocess = transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#     ])\n    \n#     return resize_preprocess\n\n\ndef small_image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset holding original database(No need to hold big image)\"\"\"\n    small_image_preprocess = transforms.Compose([\n        transforms.Resize(1),\n        transforms.ToTensor()\n    ])\n    \n    return small_image_preprocess\n\n### from here to below will change to Tensor\ndef image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset \"\"\"\n    image_preprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor()\n    ])\n    \n    return image_preprocess\n\n\n\ndef train_preprocess():\n    \"\"\"resize_preprocess() + couple transformation to improve accuracy in training, ToTensor and Normalization\"\"\"\n    transforms_train = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.RandomHorizontalFlip(p=0.5),\n        # Somehow this doesn't work\n        #transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio = (0.33, 0.33), value= 0, inplace = False), \n        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n        transforms.RandomRotation(degrees=10),\n        transforms.RandomPosterize(bits=2),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)        \n\n\n\n    ])\n    return transforms_train\n    \ndef validation_preprocess():\n    \"\"\" image_preprocess() + Normalization\"\"\"\n    validation_train = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)\n    ])\n    return validation_train\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:50.747152Z","iopub.execute_input":"2023-01-31T06:34:50.748542Z","iopub.status.idle":"2023-01-31T06:34:50.757340Z","shell.execute_reply.started":"2023-01-31T06:34:50.748505Z","shell.execute_reply":"2023-01-31T06:34:50.756450Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"code","source":"# Datasets Classes\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"create KenyanFood Dataset from annotations_file and images\"\"\"\n    def __init__(self, transform = None):\n        self.img_labels = pd.read_csv(tc.annotation_file)\n        self.img_dir = tc.img_dir\n        # Make list of the classes list will give index (iterable) -> 13 classes\n        self.classes = list(self.img_labels['class'].unique()) \n        # Make dictionary set of the classes \n        self.dict_classes = dict(enumerate(self.img_labels['class'].unique()))\n        self.transform = transform\n        \n        # weigths of each class normalized (total = 1.0)\n        def _inverse_ratio():\n            \"\"\"calculate inverse ratio of number of each classes for weighted sampler usage\"\"\"\n            series = self.img_labels['class'].value_counts()\n            total = series.sum()\n            for index, _ in enumerate(series):\n                series.iloc[index] = series.iloc[index]/total\n            return series\n        \n        # This inner method won't work for inheritance since it is inside __init__()\n        def _calc_sample_weights():\n            class_counts = self.img_labels['class'].value_counts()\n            sample_weights = [1/class_counts[i] for i in self.img_labels['class']]\n            return sample_weights\n        \n        # sample_weights\n        self.samples_weights = [self.img_labels['class'].value_counts()[i] for i in self.img_labels['class']]\n        \n        \n    def _get_img_labels(self):\n        return self.img_labels  \n        \n    def __len__(self):\n        return len(self.img_labels)\n    \n    def __getitem__(self, index):\n        \"\"\"Return (image, target) after resizing and preprocessing \n        iloc[index, 0] will return ids (e.g.,14278962987112149800) of each row\"\"\"\n        #print(f\"index: {index} \\n len of self.img_labels from __getitem__:{len(self.img_labels)}\")\n        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[index, 0])+\".jpg\")\n#         print(f\"Path created:{img_path}\")        \n        image = Image.open(img_path)\n#         print(f\"image found:{image}\")  \n        # label is string so will return index (pytorch cannot make string to tensor)\n        # iloc[index, 1] will return the class_name (e.g., githeri) for id in [index, 1] (e.g., 14278962987112149800)\n        label_index = self.class2index(self.img_labels.iloc[index, 1])  # returns int\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label_index #image: Image or Tensor ,  label_index: int\n    \n    \n    def class2index(self, class_name:str)-> int:\n        \"\"\"Returns the index of a given class.\"\"\"\n        return self.classes.index(class_name)\n    \n    \n    def index2class(self, class_index:int)-> str:\n        \"\"\"Returns the class of a given index.\"\"\"\n        return self.classes[class_index]\n\n\nclass KenyanFood13SplitDataset(KenyanFood13Dataset):\n    def __init__(self, transform = None, train= True):\n        super().__init__(transform = transform)\n        #self.img_labels = None\n#         print(f\"self.img_labels in __init()__ first: {self.img_labels}\\n\") # 6536\n        image_labels = super()._get_img_labels()\n\n        if train:\n            self.img_labels = image_labels.iloc[:int(len(image_labels) * tc.train_split)]\n#             print(f\"len of self.img_labels in __init__ () of subclass should be 5228 :{len(self.img_labels)}\")# 5228\n        else:\n            self.img_labels = image_labels.iloc[int(len(image_labels) * tc.train_split):]\n\n#         print(f\"self.img_labels in __init()__ last subclass should be 5228:  {len(self.img_labels)}\")#  5228\n        \n        # Override the sample_weights since train database has splitted database, needed for weighted Random Sampler\n        self.samples_weights = [self.img_labels['class'].value_counts()[i] for i in self.img_labels['class']]\n    \nclass KenyanFood13Testset(Dataset):\n    \"\"\"Kenyan food test dataset, contains original KenanFood13Dataset for image2class and class2image methods\"\"\"\n    def __init__(self, transform = image_preprocess_transforms()):\n        self.img_dir = tc.img_dir\n        self.test_labels = pd.read_csv(tc.test_csv_file)\n        self.transform = transform\n        # this is to use class2index() and class2index()\n        self.base_dataset = KenyanFood13Dataset(small_image_preprocess_transforms())\n\n    def __getitem__(self, index):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        \n        # get img path from test_labels. img_dir + id in csv file (per index) + \".jpg\" \n        img = os.path.join(self.img_dir, str(self.test_labels.iloc[index, 0]) + '.jpg')\n        \n        image = Image.open(img)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n    \n    def __len__(self):\n        return len(self.test_labels)","metadata":{"id":"wmEnYe40RWSj","executionInfo":{"status":"ok","timestamp":1672695586248,"user_tz":-540,"elapsed":237,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-01-31T06:34:50.821651Z","iopub.execute_input":"2023-01-31T06:34:50.821981Z","iopub.status.idle":"2023-01-31T06:34:50.840329Z","shell.execute_reply.started":"2023-01-31T06:34:50.821952Z","shell.execute_reply":"2023-01-31T06:34:50.839226Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{"id":"oG7W87aVRWSk"}},{"cell_type":"code","source":"# Settings and configurations\n@dataclass\nclass SystemConfiguration:\n    seed:int = 21\n        \n# Specifiy all the data needed in dataclass named TrainingConfiguration\n@dataclass\nclass TrainingConfiguration:\n    batch_size: int = 52\n    epoch_count: int = 20\n    init_learning_rate: float = 0.001\n    log_interval: int = 5\n    test_interval: int = 1\n    data_root: str = \"/kaggle/input/opencv-pytorch-dl-course-classification/\"\n    model_dir: str = '/kaggle/working/models/'\n    log_dir: str = '/kaggle/working/logs'\n    annotation_file: str = data_root + \"train.csv\"\n    test_csv_file: str = data_root + \"test.csv\"\n    img_dir: str = data_root + \"images/images/\"\n    submission_csv: str = '/kaggle/working/submission.csv'\n    num_workers: int = 2\n    device: str = 'cuda'\n    train_split: float = 0.8\n    random_seed: int = 42\n    model_name: str = \"resnet152\"\n    lr: float =0.00001\n    classes: tuple = ('githeri', 'ugali', 'kachumbari', 'matoke', 'sukumawiki', 'bhaji', 'mandazi', 'kukuchoma', 'nyamachoma', 'pilau', 'chapati', 'masalachips', 'mukimo')\n    dataset_len: int = 6536\n    #below is calculated from KenyanFood13 pictures\n    #mean: torch.tensor = torch.tensor([0.5768, 0.4622, 0.3460])\n    #std: torch.tensor = torch.tensor([0.2699, 0.2739, 0.2826])\n    \n    #below mean and standard are for Resnet18 https://pytorch.org/hub/pytorch_vision_resnet/\n    mean: torch.tensor = torch.tensor([0.485, 0.456, 0.406])\n    std: torch.tensor  = torch.tensor([0.229, 0.224, 0.225])\n        \n    tb_writer: SummaryWriter = SummaryWriter(log_dir)\n    criterion: torch.nn.CrossEntropyLoss = torch.nn.CrossEntropyLoss()        \n\ndef setup_system(SystemConfiguration):\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.backends.cudnn_benchmark_enabled = True\n        torch.backends.cudnn.deterministic = True\n        \n        \ntc = TrainingConfiguration()\nsystem_config = SystemConfiguration()\nsetup_system(system_config)\n\n\nfor path in [tc.log_dir, tc.model_dir]:\n    if not os.path.exists(path):\n        os.makedirs(path)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-31T07:03:13.232745Z","iopub.execute_input":"2023-01-31T07:03:13.233114Z","iopub.status.idle":"2023-01-31T07:03:13.249801Z","shell.execute_reply.started":"2023-01-31T07:03:13.233085Z","shell.execute_reply":"2023-01-31T07:03:13.248850Z"},"trusted":true},"execution_count":237,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"372d290ERWSl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{"id":"XtEdfRZjRWSl"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{"id":"qqXcmrqbRWSm"}},{"cell_type":"code","source":"def train(model, train_loader, optimizer) -> float:\n    \"\"\"train by model by given train_loader(and its datasets) and optimizer, return average loss and accuracy\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.train()\n    \n    for inputs, labels in tqdm(train_loader):\n        count += len(labels)\n        # move to target device (GPU or CPU)\n        inputs = inputs.to(tc.device)\n        labels = labels.to(tc.device)\n        \n        # prediction\n        outputs = model(inputs)\n        # calculate loss\n        loss = tc.criterion(outputs, labels)\n        # initialize the gradients -> calucalte the gradients -> Update the gradients\n        optimizer.zero_grad()     \n        loss.backward()\n        optimizer.step()\n        \n        # adds up loss pf number of batches\n        total_loss += loss.item()\n        # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n        avg_loss = total_loss / count\n        \n        # get highest predicted value\n        predicted = torch.max(outputs, axis=1)[1]\n        # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n        total_acc  += (predicted == labels).sum().item()\n        avg_acc =  total_acc / count\n        \n    return avg_loss, avg_acc","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:50.867196Z","iopub.execute_input":"2023-01-31T06:34:50.869387Z","iopub.status.idle":"2023-01-31T06:34:50.878324Z","shell.execute_reply.started":"2023-01-31T06:34:50.869360Z","shell.execute_reply":"2023-01-31T06:34:50.877482Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"def validate(model, validation_loader):\n    \"\"\"validate by model by given validation_loader(and its datasets) , return average loss\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(validation_loader):\n            count += len(labels)\n            # move to target device (GPU or CPU)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # prediction\n            outputs = model(inputs)\n            # calculate loss\n            loss = tc.criterion(outputs, labels)\n            \n            # adds up loss pf number of batches\n            total_loss += loss.item()\n            # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n            avg_loss = total_loss / count\n            \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            total_acc  += (predicted == labels).sum().item()\n            # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n            avg_acc =  total_acc / count\n        \n            \n    return avg_loss, avg_acc\n        ","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:50.880370Z","iopub.execute_input":"2023-01-31T06:34:50.880694Z","iopub.status.idle":"2023-01-31T06:34:50.892284Z","shell.execute_reply.started":"2023-01-31T06:34:50.880663Z","shell.execute_reply":"2023-01-31T06:34:50.891333Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"def predict(trained_model, prediction_loader) -> list:\n    \"\"\"prediction with model and test_loader, returns list of prediction result, assume to be used for inference ONLY\"\"\"  \n    predictions = torch.tensor([]).to(tc.device)\n    for inputs in prediction_loader:\n        inputs = inputs.to(tc.device)\n        outputs = trained_model(inputs)\n        # get the predicted result by index number, each prediction will be done by tc.batch_size so predicted is array of batch_size\n        predictions = torch.hstack((predictions, torch.max(outputs, axis=1)[1]))\n        \n    return predictions.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:50.895372Z","iopub.execute_input":"2023-01-31T06:34:50.895631Z","iopub.status.idle":"2023-01-31T06:34:50.907706Z","shell.execute_reply.started":"2023-01-31T06:34:50.895608Z","shell.execute_reply":"2023-01-31T06:34:50.906760Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{"id":"0-ysifviRWSm"}},{"cell_type":"code","source":"# # Resnet 18 (or 152) and change last layer output to 13\n# model = resnet18(pretrained = True)\n# # Change the last output to 13\n# fc_in_features = model.fc.in_features\n# model.fc = torch.nn.Linear(fc_in_features, (len(tc.classes)))\n# model = model.to(tc.device)","metadata":{"id":"fRNx53rwRWSn","execution":{"iopub.status.busy":"2023-01-31T07:09:12.926142Z","iopub.execute_input":"2023-01-31T07:09:12.926522Z","iopub.status.idle":"2023-01-31T07:09:13.154213Z","shell.execute_reply.started":"2023-01-31T07:09:12.926492Z","shell.execute_reply":"2023-01-31T07:09:13.153202Z"},"trusted":true},"execution_count":246,"outputs":[]},{"cell_type":"code","source":"# Efficientnet and change last layer output to 13\n!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\n\nmodel = EfficientNet.from_pretrained('efficientnet-b7')\nfc_in_features = model._fc.in_features\nmodel.fc = torch.nn.Linear(fc_in_features, (len(tc.classes)))\nmodel = model.to(tc.device)","metadata":{"id":"7mYFpjLQRWSn","execution":{"iopub.status.busy":"2023-01-31T07:07:55.838704Z","iopub.execute_input":"2023-01-31T07:07:55.839087Z","iopub.status.idle":"2023-01-31T07:08:07.425660Z","shell.execute_reply.started":"2023-01-31T07:07:55.839055Z","shell.execute_reply":"2023-01-31T07:08:07.424289Z"},"trusted":true},"execution_count":243,"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.7/site-packages (0.7.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.11.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (4.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLoaded pretrained weights for efficientnet-b7\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{"id":"mwFF7LkBRWSn"}},{"cell_type":"code","source":"## util functions\n\ndef get_mean_std(dataset):\n    \"\"\"returns mean and standard deviation of dataset given, since this method will take long time, once calcuated, not been used\"\"\"\n    # calculated mean: tensor([0.5768, 0.4622, 0.3460]), std:tensor([0.2699, 0.2739, 0.2826]):\n    loader = DataLoader(dataset)\n    \n    batch_mean = torch.zeros(3) # tensor([0,0,0])\n    batch_mean_sqrd = torch.zeros(3)\n    \n    for batch_data, _ in loader:\n        batch_mean += batch_data.mean(dim=(0,2,3))\n        batch_mean_sqrd += (batch_data **2).mean(dim=(0,2,3))\n        \n    mean = batch_mean / len(loader)\n    var = (batch_mean_sqrd) / len(loader) - (mean **2)\n    \n    std = var ** .5\n    \n    print(\"mean: {}, std:{}:\".format(mean, std))\n    return mean, std\n\n\ndef eval_loss(data_loader, device, model, criterion):\n    \"\"\"evaluate losses and can use this for visualize\"\"\"\n    for images, labales in data_loader:\n        break\n        \n    inputs = images.to(device)\n    labels = labels.to(device)\n    \n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    \n    return loss\n\ndef evaluate_history(history):\n    import matplotlib.pyplot as plt\n    print(\"initial stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[0,3], history[0,4]))\n    print(\"final stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[-1,3], history[-1,4]))\n    \n    num_epochs = len(history)\n    unit = num_epochs/10\n    \n    # Display learning curve (Loss)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,1], 'b', label='train')\n    plt.plot(history[:,0], history[:,3], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1, unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Loss)')\n    plt.legend()\n    plt.show()\n\n    # Display learning curve (Accuracy)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,2], 'b', label='train')\n    plt.plot(history[:,0], history[:,4], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1,unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Accuracy')\n    plt.legend()\n    plt.show()\n\n\n# model save and load functions\ndef save_model(model, device):\n    if not os.path.exists(tc.model_dir):\n        os.makedirs(tc.model_dir)\n    \n    model_path = os.path.join(tc.model_dir, tc.model_name)\n    \n    if device == \"cuda\":\n        model.to(\"cpu\")\n    \n    torch.save(model.state_dict(), model_path + \"best_model.pt\")\n    \n    if device == \"cuda\":\n        model.to(\"cuda\")\n    \n    return\n\ndef load_model(model):\n    \n    model_path = os.path.join(tc.model_dir, tc.model_name)\n    model.load_state_dict(torch.load(model_path + 'best_model.pt'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:52.183883Z","iopub.execute_input":"2023-01-31T06:34:52.184184Z","iopub.status.idle":"2023-01-31T06:34:52.201075Z","shell.execute_reply.started":"2023-01-31T06:34:52.184159Z","shell.execute_reply":"2023-01-31T06:34:52.199952Z"},"trusted":true},"execution_count":233,"outputs":[]},{"cell_type":"markdown","source":"* id: 0 'githeri': 479,\n* id: 1 'ugali': 628,\n* id: 2 'kachumbari': 494,\n* id: 3 'matoke': 483,\n* id: 4 'sukumawiki': 402,\n* id: 5 'bhaji': 632,\n* id: 6 'mandazi': 620,\n* id: 7 'kukuchoma': 173,\n* id: 8 'nyamachoma': 784,\n* id: 9 'pilau': 329,\n* id:10 'chapati': 862,\n* id:11 'masalachips': 438,\n* id:13 'mukimo': 212","metadata":{}},{"cell_type":"code","source":"from torch.utils.data.sampler import BatchSampler\n\nclass BalancedBatchSampler(BatchSampler):\n    \n    \n    def __init__(self, dataset, n_classes, n_samples):\n        loader = DataLoader(dataset)\n        # keep ALL the label as list\n        self.labels_list = []\n        for _, label in loader:\n            self.labels_list.append(label)\n        # and keep it as Tensor\n        self.labels = torch.LongTensor(self.labels_list)\n        \n        self.labels_set = list(set(self.labels.numpy()))\n        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n                                 for label in self.labels_set}\n        for l in self.labels_set:\n            np.random.shuffle(self.label_to_indices[l])\n        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n        self.count = 0\n        self.n_classes = n_classes\n        self.n_samples = n_samples\n        self.dataset = dataset\n        self.batch_size = self.n_samples * self.n_classes\n        \n    def __iter__(self):\n        self.count = 0\n        while self.count + self.batch_size < len(self.dataset):\n            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n            indices = []\n            for class_ in classes:\n                indices.extend(self.label_to_indices[class_][\n                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n                                                                         class_] + self.n_samples])\n                self.used_label_indices_count[class_] += self.n_samples\n                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n                    np.random.shuffle(self.label_to_indices[class_])\n                    self.used_label_indices_count[class_] = 0\n            yield indices\n            self.count += self.n_classes * self.n_samples\n\n    def __len__(self):\n        return len(self.dataset) // self.batch_size","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:52.203080Z","iopub.execute_input":"2023-01-31T06:34:52.203885Z","iopub.status.idle":"2023-01-31T06:34:52.218974Z","shell.execute_reply.started":"2023-01-31T06:34:52.203848Z","shell.execute_reply":"2023-01-31T06:34:52.217859Z"},"trusted":true},"execution_count":234,"outputs":[]},{"cell_type":"code","source":"def visualize_batch_data(dataloader, with_visual_chart = True , with_class_string = True) -> dict:\n    \"\"\"visualize what class is in the batch to visually see the equalness of the samples\"\"\"\n    #{0: 'githeri', 1: 'ugali', 2: 'kachumbari', 3: 'matoke', 4: 'sukumawiki', 5: 'bhaji', 6: 'mandazi', 7: 'kukuchoma', 8: 'nyamachoma', 9: 'pilau', 10: 'chapati', 11: 'masalachips', 12: 'mukimo'}\n    # total_number_appeared: Counter({10: 862, 8: 784, 5: 632, 1: 628, 6: 620, 2: 494, 3: 483, 0: 479, 11: 438, 4: 402, 9: 329, 12: 212, 7: 173})\n    total_num_images = len(data_loader.dataset)\n    total_number_appeared = collections.Counter()\n    number_appeared_per_batch = []\n    dict_classes = data_loader.dataset.dict_classes\n    \n    for index, (_, label_index) in enumerate(data_loader): # don't use image\n        class_ids, class_counts = torch.unique(label_index, return_counts=True) # return in torch.tensor\n        number_appeared = collections.Counter(label_index.tolist())\n        print(f\"batch:{index} number_appeared:{number_appeared}\")\n        \n        # add zero for sample which was not picked, and add that label_index:0 to Counter\n        while len(number_appeared) < len(dict_classes):\n            for i in range(len(number_appeared)):\n                number_appeared.setdefault(i,0)\n            print(f\"batch:{index} number_appeared_after_setdefault:{number_appeared}\")\n        # for total count\n        total_number_appeared += number_appeared\n\n            \n        # sort based on keys (label_index) and cast to dict for visualization\n        dict_number_appeared = dict(sorted(number_appeared.items()))\n        #print(f\"dict_number_appeared:{dict_number_appeared}\")\n        number_appeared_per_batch.append(dict_number_appeared)\n        \n    if with_visual_chart:\n        \"\"\"create bar graph for all batches which class had samples\"\"\"\n        fig, axes = plt.subplots(int(tc.batch_size/4), 4 , figsize = (20, 20))\n        for i in range(int(tc.batch_size/4)):\n            for j in range(4):\n                axes[i][j].bar(number_appeared_per_batch[int(j+(i*4))].keys(), list(number_appeared_per_batch[int(j+(i*4))].values()))\n                if with_class_string:\n                    axes[i][j].set_xticks(list(range(len(dict_classes))))\n                    axes[i][j].set_xticklabels(list(dict_classes.values()))\n                else:\n                    axes[i][j].set_xticks(list(range(len(dict_classes))))\n                    axes[i][j].set_xticklabels(list(dict_classes.keys()))\n\n    \n    # return dict type, key=class name, value=how many times appeared\n    if with_class_string:\n        return {data_loader.dataset.index2class(key):value for key, value in total_number_appeared.items()}\n    \n    # return dict type, key=class_id, value=how many times appeared\n    return dict(total_number_appeared)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:34:52.221689Z","iopub.execute_input":"2023-01-31T06:34:52.222438Z","iopub.status.idle":"2023-01-31T06:34:52.237335Z","shell.execute_reply.started":"2023-01-31T06:34:52.222395Z","shell.execute_reply":"2023-01-31T06:34:52.235843Z"},"trusted":true},"execution_count":235,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{"id":"tyZ0aRAaRWSn"}},{"cell_type":"code","source":"# split dataset into train & validation dataset based on split ratio\nkenyan_dataset = KenyanFood13Dataset() \ntrain_dataset = KenyanFood13SplitDataset(transform = train_preprocess(),train = True)\nvalidation_dataset = KenyanFood13SplitDataset(transform = validation_preprocess(), train = False)\nsequential_sampler = torch.utils.data.SequentialSampler(train_dataset)\nweight_sampler = WeightedRandomSampler(weights=train_dataset.samples_weights, num_samples=len(train_dataset), replacement=True)\ntrain_loader = DataLoader(train_dataset, sampler = weight_sampler, batch_size = tc.batch_size, shuffle = False)\nvalidation_loader = DataLoader(validation_dataset, batch_size = tc.batch_size, shuffle = False )\n\nprint(f\"length of train_dataset {len(train_dataset.samples_weights)}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-31T07:09:21.482318Z","iopub.execute_input":"2023-01-31T07:09:21.482678Z","iopub.status.idle":"2023-01-31T07:09:35.027500Z","shell.execute_reply.started":"2023-01-31T07:09:21.482646Z","shell.execute_reply":"2023-01-31T07:09:35.026522Z"},"trusted":true},"execution_count":247,"outputs":[{"name":"stdout","text":"length of train_dataset 5228\n","output_type":"stream"}]},{"cell_type":"code","source":"best_val_loss = np.Inf\nhistory = np.zeros((0,6))\nfor epoch in range(tc.epoch_count):\n    # time one cycle of train and validate\n    start_time = time.time()\n    train_loss, train_acc = train(model, train_loader, optimizer)\n    val_loss, val_acc = validate(model, validation_loader)\n    elapsed_time = time.time() - start_time\n    \n    # save the weight when the loss gets smaller than last one\n    if val_loss < best_val_loss:\n        save_model(model, tc.device)\n        print(\"WEIGHTS-ARE-SAVED\")\n        best_val_loss = val_loss\n        \n    print(f\"Epoch{epoch+1}/{tc.epoch_count}, loss:{train_loss:.5f}, acc:{train_acc:.5f}, val_loss:{val_loss:.5f}, val_acc:{val_acc:.5f}, elapsed_time:{elapsed_time:.5f}\")\n    # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n    item = np.array([epoch+1, train_loss, train_acc, val_loss, val_acc, elapsed_time])\n    # store history for matplotlib visualization\n    history = np.vstack((history, item))\n\n    # For tensorboard\n    tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_loss, epoch+1)\n    tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_acc, epoch+1)\n    tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T07:09:36.931914Z","iopub.execute_input":"2023-01-31T07:09:36.932374Z","iopub.status.idle":"2023-01-31T07:09:38.792481Z","shell.execute_reply.started":"2023-01-31T07:09:36.932333Z","shell.execute_reply":"2023-01-31T07:09:38.790883Z"},"trusted":true},"execution_count":248,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"657a76d76d9647b985e5448913f2bc2f"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/1593897045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# time one cycle of train and validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/1468861688.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2422\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m     )\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 15.90 GiB total capacity; 14.45 GiB already allocated; 81.75 MiB free; 14.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 15.90 GiB total capacity; 14.45 GiB already allocated; 81.75 MiB free; 14.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now do the inference and get submission.csv file\ntest_dataset = KenyanFood13Testset()\ntest_loader = DataLoader(test_dataset, batch_size = tc.batch_size, shuffle = False)\n\n# prediction\npredicted_result = predict(model, test_loader).tolist()\n#print(\"predicted_result len:{}\".format(len(predicted_result)))\nclassifications = []\n\n#change predicted index to class_names\nfor index in range(len(predicted_result)):\n    predicted_result_class = test_dataset.base_dataset.index2class(int(predicted_result[index]))\n    classifications.append(predicted_result_class)\n\n# put it to csv file\nclasses = pd.DataFrame(classifications, columns = [\"class\"])\nresult = test_dataset.test_labels.join(classes)\nresult.to_csv(tc.submission_csv, index=False, header = True)","metadata":{"id":"WqFnOpy1RWSn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"f7bNvOcZRWSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(model, optimizer, train_loader, validation_loader, history):\n    \"\"\"works like tensorflow fits. epoch_count, optimizer and criterion(e.g., CrossEntorpyLoss) from training config\"\"\"\n    \n    #from tqdm.notebook import tqdm\n    \n    base_epochs = len(history)\n    \n    for epoch in range(base_epochs, tc.epoch_count + base_epochs):\n        train_loss = 0\n        train_acc = 0\n        val_loss = 0\n        val_acc = 0\n        best_valid_loss = np.Inf\n        \n        # training Phase\n        model.train()\n        count = 0\n        \n        t_begin = time.time()\n        for inputs, labels in train_loader:\n            \n            count += len(labels)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # initialize the gradients\n            optimizer.zero_grad()\n            # predict\n            outputs = model(inputs)\n            # calculate loss\n            loss = tc.criterion(outputs, labels)\n            train_loss += loss.item()\n            \n            #calucalte the graidents\n            loss.backward()\n            #update parameter\n            optimizer.step()\n            \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            train_acc  += (predicted == labels).sum().item()\n            \n            #calculate the average loss and accuracy in the batch\n            avg_train_loss = train_loss / count\n            avg_train_acc = train_acc / count\n            \n        #evaluation phase\n        model.eval()\n        count = 0\n        \n        for inputs, labels in validation_loader:\n            count += len(labels)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # calculate the prediction\n            outputs = model(inputs)\n            \n            # calculate the loss\n            loss = tc.criterion(outputs, labels)\n            val_loss += loss.item()\n            \n            if val_loss < best_valid_loss:\n                torch.save(model.state_dict(), \"best_model.pt\")\n                print(\"WEIGHTS-ARE-SAVED\")\n                best_valid_loss = val_loss\n                \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            val_acc += (predicted == labels).sum().item()\n            \n            #calculate the average loss and accuracy in the batch\n            avg_val_loss = val_loss / count\n            avg_val_acc = val_acc / count\n            \n            \n        elapsed_time = time.time() - t_begin\n        # print out\n        print(\"Epoch{}/{},  loss:{:.5f}, acc:{:.5f}, val_loss:{:.5f}, val_acc:{:.5f}, elapsed_time:{:.5f}\".format(\n            epoch+1,tc.epoch_count+base_epochs, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time\n        ))\n        # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n        item = np.array([epoch+1, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time])\n        history = np.vstack((history, item))\n        \n        # For tensorboard\n        tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_loss, epoch+1)\n        tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_acc, epoch+1)\n        tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)\n\n        \n    return history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size = tc.batch_size, shuffle = True)\nvalidation_loader = DataLoader(validation_dataset, batch_size = tc.batch_size, shuffle = False)\nhistory = np.zeros((0,6)) # history to record 0: num of iteration, 1:training_loss, 2:trainin_acc, 3:val_loss, 4:val_acc 5:elapsed time\noptimizer = SGD(model.parameters(), lr=tc.lr, momentum = 0.9)\nhistory = fit(model, optimizer, train_loader, validation_loader, history)","metadata":{"id":"9vg8rIMjRWSm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n\n**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\nFor example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars).","metadata":{"id":"QRbjuol1RWSo"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"id":"NHuqwjtrRWSo"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{"id":"zP0rAnxrRWSo"}}]}