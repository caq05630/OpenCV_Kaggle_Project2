{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{"id":"rVHwWejfRWSd"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n    \n```\n\n```\ndef get_data(args1, *agrs):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{"id":"EI9ivVwbRWSi"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">Class_counts: (very uneven)</font>\n* id: 0 'githeri': 479,\n* id: 1 'ugali': 628,\n* id: 2 'kachumbari': 494,\n* id: 3 'matoke': 483,\n* id: 4 'sukumawiki': 402,\n* id: 5 'bhaji': 632,\n* id: 6 'mandazi': 620,\n* id: 7 'kukuchoma': 173,\n* id: 8 'nyamachoma': 784,\n* id: 9 'pilau': 329,\n* id:10 'chapati': 862,\n* id:11 'masalachips': 438,\n* id:13 'mukimo': 212\n\nName: class, dtype: int64, sum: 6536\n\n","metadata":{}},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:19:17.862787Z","iopub.execute_input":"2023-02-15T03:19:17.863489Z","iopub.status.idle":"2023-02-15T03:19:17.871242Z","shell.execute_reply.started":"2023-02-15T03:19:17.863452Z","shell.execute_reply":"2023-02-15T03:19:17.870040Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Import neccesaary libraries\nimport os\nimport time\nimport collections\nfrom dataclasses import dataclass\n\n# third party library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n\n# Pytorch related\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import WeightedRandomSampler\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Subset\nfrom torch.optim import SGD  \nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom torchvision.models import resnet152, resnet18","metadata":{"id":"xlzaxohURWSi","executionInfo":{"status":"ok","timestamp":1672696976022,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-02-15T03:19:19.102938Z","iopub.execute_input":"2023-02-15T03:19:19.105226Z","iopub.status.idle":"2023-02-15T03:19:19.112469Z","shell.execute_reply.started":"2023-02-15T03:19:19.105178Z","shell.execute_reply":"2023-02-15T03:19:19.111316Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# all the Transforms\n# def resize_preprocess():\n#     \"\"\"Compulsory transforms image to same_size and center cropped (not changing to Tensor yet)\"\"\"\n#     resize_preprocess = transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#     ])\n    \n#     return resize_preprocess\n\n\ndef small_image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset holding original database(No need to hold big image)\"\"\"\n    small_image_preprocess = transforms.Compose([\n        transforms.Resize(1),\n        transforms.ToTensor()\n    ])\n    \n    return small_image_preprocess\n\n\n### from here to below will change to Tensor\ndef image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset \"\"\"\n    image_preprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor()\n    ])\n    \n    return image_preprocess\n\n\ndef train_preprocess():\n    \"\"\"resize_preprocess() + couple transformation to improve accuracy in training, ToTensor and Normalization\"\"\"\n    transforms_train = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.RandomHorizontalFlip(p=0.5),\n        # Somehow this doesn't work\n        #transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio = (0.33, 0.33), value= 0, inplace = False), \n        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n        transforms.RandomRotation(degrees=10),\n        transforms.RandomPosterize(bits=2),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)        \n    ])\n    return transforms_train\n    \n    \ndef validation_preprocess():\n    \"\"\" image_preprocess() + Normalization\"\"\"\n    validation_train = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)\n    ])\n    return validation_train\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:19:21.150864Z","iopub.execute_input":"2023-02-15T03:19:21.151531Z","iopub.status.idle":"2023-02-15T03:19:21.162596Z","shell.execute_reply.started":"2023-02-15T03:19:21.151495Z","shell.execute_reply":"2023-02-15T03:19:21.161634Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Datasets Classes\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"create KenyanFood Dataset from annotations_file and images\"\"\"\n    def __init__(self, transform = None):\n        self.img_labels = pd.read_csv(tc.annotation_file)\n        self.img_dir = tc.img_dir\n        # Make list of the classes list will give index (iterable) -> 13 classes\n        self.classes = list(self.img_labels['class'].unique()) \n        # Make dictionary set of the classes \n        self.dict_classes = dict(enumerate(self.img_labels['class'].unique()))\n        self.transform = transform\n        \n        # weigths of each class normalized (total = 1.0)\n        def _inverse_ratio():\n            \"\"\"calculate inverse ratio of number of each classes for weighted sampler usage\"\"\"\n            series = self.img_labels['class'].value_counts()\n            total = series.sum()\n            for index, _ in enumerate(series):\n                series.iloc[index] = series.iloc[index]/total\n            return series\n        \n        # This inner method won't work for inheritance since it is inside __init__()\n        def _calc_sample_weights():\n            class_counts = self.img_labels['class'].value_counts()\n            sample_weights = [1/class_counts[i] for i in self.img_labels['class']]\n            return sample_weights\n        \n        # sample_weights\n        self.samples_weights = [self.img_labels['class'].value_counts()[i] for i in self.img_labels['class']]       \n    \n    # need this method to get original dataset image_labels from child class\n    def _get_img_labels(self):\n        return self.img_labels  \n        \n    def __len__(self):\n        return len(self.img_labels)\n    \n    def __getitem__(self, index):\n        \"\"\"Return (image, target) after resizing and preprocessing \n        iloc[index, 0] will return ids (e.g.,14278962987112149800) of each row\"\"\"\n        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[index, 0])+\".jpg\")     \n        image = Image.open(img_path)\n\n        # label is string so will return index (pytorch cannot make string to tensor)\n        # iloc[index, 1] will return the class_name (e.g., githeri) for id in [index, 1] (e.g., 14278962987112149800)\n        label_index = self.class2index(self.img_labels.iloc[index, 1])  # returns int\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label_index #image: Image or Tensor ,  label_index: int -> Does this needs to be Tensor?\n    \n    \n    def class2index(self, class_name:str)-> int:\n        \"\"\"Returns the index of a given class.\"\"\"\n        return self.classes.index(class_name)\n    \n    \n    def index2class(self, class_index:int)-> str:\n        \"\"\"Returns the class of a given index.\"\"\"\n        return self.classes[class_index]\n\n\nclass KenyanFood13SplitDataset(KenyanFood13Dataset):\n    def __init__(self, transform = None, train= True):\n        # above __init__() will override super class init(), so needs to call super()__init__ for initialization\n        super().__init__(transform = transform)\n        #self.img_labels = None\n#       print(f\"self.img_labels in __init()__ first: {self.img_labels}\\n\") # 6536\n\n        # need this method to get original dataset image_labels\n        image_labels = super()._get_img_labels()\n\n        # Overrides self.img_labels by ratio of split\n        if train:\n            self.img_labels = image_labels.iloc[:int(len(image_labels) * tc.train_split)]\n        else: #for validation\n            self.img_labels = image_labels.iloc[int(len(image_labels) * tc.train_split):]\n        \n        # Override the sample_weights since train database has splitted database, needed for weighted Random Sampler\n        self.samples_weights = [self.img_labels['class'].value_counts()[i] for i in self.img_labels['class']]\n    \nclass KenyanFood13Testset(Dataset):\n    \"\"\"Kenyan food test dataset, contains original KenanFood13Dataset for image2class and class2image methods\"\"\"\n    def __init__(self, transform = image_preprocess_transforms()):\n        self.img_dir = tc.img_dir\n        self.test_labels = pd.read_csv(tc.test_csv_file)\n        self.transform = transform\n        # this is to use class2index() and class2index()\n        self.base_dataset = KenyanFood13Dataset(small_image_preprocess_transforms())\n\n    def __getitem__(self, index):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        \n        # get img path from test_labels. img_dir + id in csv file (per index) + \".jpg\" \n        img = os.path.join(self.img_dir, str(self.test_labels.iloc[index, 0]) + '.jpg')\n        \n        image = Image.open(img)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n    \n    def __len__(self):\n        return len(self.test_labels)","metadata":{"id":"wmEnYe40RWSj","executionInfo":{"status":"ok","timestamp":1672695586248,"user_tz":-540,"elapsed":237,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-02-15T03:19:23.623323Z","iopub.execute_input":"2023-02-15T03:19:23.623697Z","iopub.status.idle":"2023-02-15T03:19:23.642775Z","shell.execute_reply.started":"2023-02-15T03:19:23.623663Z","shell.execute_reply":"2023-02-15T03:19:23.641578Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{"id":"oG7W87aVRWSk"}},{"cell_type":"code","source":"# Settings and configurations\n@dataclass\nclass SystemConfiguration:\n    seed:int = 42\n        \n# Specifiy all the data needed in dataclass named TrainingConfiguration\n@dataclass\nclass TrainingConfiguration:\n    batch_size: int = 13\n    epoch_count: int = 200\n    init_learning_rate: float = 0.00001\n    log_interval: int = 5\n    test_interval: int = 1\n    data_root: str = \"/kaggle/input/opencv-pytorch-dl-course-classification/\"\n    model_dir: str = '/kaggle/working/models/'\n    log_dir: str = '/kaggle/working/logs'\n    annotation_file: str = data_root + \"train.csv\"\n    test_csv_file: str = data_root + \"test.csv\"\n    img_dir: str = data_root + \"images/images/\"\n    submission_csv: str = '/kaggle/working/submission.csv'\n    num_workers: int = 2\n    device: str = 'cpu'\n    train_split: float = 0.8\n    random_seed: int = 42\n    model_name: str = \"resnet152\"\n    res18_layers :tuple = (2,2, 2,2)\n    res34_layers :tuple = (3,4, 6,3)\n    res50_layers :tuple = (3,4, 6,3)\n    res101_layers:tuple = (3,4,23,3)\n    res152_layers:tuple = (3,8,36,3)\n    \n    lr: float =0.00001\n    classes: tuple = ('githeri', 'ugali', 'kachumbari', 'matoke', 'sukumawiki', 'bhaji', 'mandazi', 'kukuchoma', 'nyamachoma', 'pilau', 'chapati', 'masalachips', 'mukimo')\n    dataset_len: int = 6536\n    #below is calculated from KenyanFood13 pictures\n    #mean: torch.tensor = torch.tensor([0.5768, 0.4622, 0.3460])\n    #std: torch.tensor = torch.tensor([0.2699, 0.2739, 0.2826])\n    \n    #below mean and standard are for Resnet18 https://pytorch.org/hub/pytorch_vision_resnet/\n    mean: torch.tensor = torch.tensor([0.485, 0.456, 0.406])\n    std: torch.tensor  = torch.tensor([0.229, 0.224, 0.225])\n        \n    tb_writer: SummaryWriter = SummaryWriter(log_dir)\n    criterion: torch.nn.CrossEntropyLoss = torch.nn.CrossEntropyLoss()        \n\ndef setup_system(SystemConfiguration):\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        tc.device = torch.device(\"cuda:0\")\n        torch.backends.cudnn_benchmark_enabled = True\n        torch.backends.cudnn.deterministic = True\n        \n        \ntc = TrainingConfiguration()\nsystem_config = SystemConfiguration()\nsetup_system(system_config)\n\nfor path in [tc.log_dir, tc.model_dir]:\n    if not os.path.exists(path):\n        os.makedirs(path)","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:19:28.599577Z","iopub.execute_input":"2023-02-15T03:19:28.599965Z","iopub.status.idle":"2023-02-15T03:19:28.618583Z","shell.execute_reply.started":"2023-02-15T03:19:28.599930Z","shell.execute_reply":"2023-02-15T03:19:28.617487Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{"id":"XtEdfRZjRWSl"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{"id":"qqXcmrqbRWSm"}},{"cell_type":"code","source":"def train(model, train_loader, optimizer) -> float:\n    \"\"\"train by model by given train_loader(and its datasets) and optimizer, return average loss and accuracy\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.train()\n    \n    for inputs, labels in tqdm(train_loader):\n        count += len(labels)\n        # move to target device (GPU or CPU)\n        inputs = inputs.to(tc.device)\n        labels = labels.to(tc.device)\n        \n        # prediction\n        outputs = model(inputs)\n        # calculate loss\n        loss = tc.criterion(outputs, labels)\n        # initialize the gradients -> calucalte the gradients -> Update the gradients\n        optimizer.zero_grad()     \n        loss.backward()\n        optimizer.step()\n        \n        # adds up loss pf number of batches\n        total_loss += loss.item()\n        # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n        avg_loss = total_loss / count\n        \n        # get highest predicted value\n        predicted = torch.max(outputs, axis=1)[1]\n        # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n        total_acc  += (predicted == labels).sum().item()\n        avg_acc =  total_acc / count\n        \n    return avg_loss, avg_acc","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:19:31.115789Z","iopub.execute_input":"2023-02-15T03:19:31.116801Z","iopub.status.idle":"2023-02-15T03:19:31.125808Z","shell.execute_reply.started":"2023-02-15T03:19:31.116750Z","shell.execute_reply":"2023-02-15T03:19:31.124599Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def validate(model, validation_loader):\n    \"\"\"validate by model by given validation_loader(and its datasets) , return average loss\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(validation_loader):\n            count += len(labels)\n            # move to target device (GPU or CPU)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # prediction\n            outputs = model(inputs)\n            # calculate loss\n            loss = tc.criterion(outputs, labels)\n            \n            # adds up loss pf number of batches\n            total_loss += loss.item()\n            # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n            avg_loss = total_loss / count\n            \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            total_acc  += (predicted == labels).sum().item()\n            # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n            avg_acc =  total_acc / count\n        \n            \n    return avg_loss, avg_acc\n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:19:34.295754Z","iopub.execute_input":"2023-02-15T03:19:34.296716Z","iopub.status.idle":"2023-02-15T03:19:34.304649Z","shell.execute_reply.started":"2023-02-15T03:19:34.296671Z","shell.execute_reply":"2023-02-15T03:19:34.303495Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def predict(trained_model, prediction_loader) -> list:\n    \"\"\"prediction with model and test_loader, returns list of prediction result, assume to be used for inference ONLY\"\"\"  \n    predictions = torch.tensor([]).to(tc.device)\n    for inputs in prediction_loader:\n        inputs = inputs.to(tc.device)\n        outputs = trained_model(inputs)\n        # get the predicted result by index number, each prediction will be done by tc.batch_size so predicted is array of batch_size\n        predictions = torch.hstack((predictions, torch.max(outputs, axis=1)[1]))\n        \n    return predictions.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:19:36.328778Z","iopub.execute_input":"2023-02-15T03:19:36.329501Z","iopub.status.idle":"2023-02-15T03:19:36.335779Z","shell.execute_reply.started":"2023-02-15T03:19:36.329466Z","shell.execute_reply":"2023-02-15T03:19:36.334707Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{"id":"0-ysifviRWSm"}},{"cell_type":"code","source":"#implement Resnet from Scratch\n# conv3x3 and conv1x1 as static method since used a lot.\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride=stride, padding=1, bias=False)\n\ndef conv1x1(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:19:40.854632Z","iopub.execute_input":"2023-02-15T03:19:40.855333Z","iopub.status.idle":"2023-02-15T03:19:40.861914Z","shell.execute_reply.started":"2023-02-15T03:19:40.855288Z","shell.execute_reply":"2023-02-15T03:19:40.860751Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    # at the last layer of each block, the channel is x1, for example resnet30 first Bottleneck,\n    # 1x1 conv 64 -> 3x3 conv 64\n    expansion = 1\n\n    def __init__(self, in_channels, base_channels, stride=1):\n        \"\"\"\n        Constructor\n\n        Parameters\n        ----------\n        in_channels : int\n            number of channels for input\n        base_channels : int\n            number of channels used in this basic block\n        stride : int\n            stride used in this basic block\n        \"\"\"\n        super().__init__()\n        self.conv1 = conv3x3(in_channels, base_channels, stride)\n        self.bn1 = nn.BatchNorm2d(base_channels)\n        self.relu = nn.ReLU(inplace=True)  # inplace=True will directory change the value\n        self.conv2 = conv3x3(base_channels, base_channels)\n        self.bn2 = nn.BatchNorm2d(base_channels)\n\n        # First residual block's number of input channel and output channel is different (e.g., conv3_x in res18 has input channel as 64 but out put is 128)\n        # And also the size (56 x 56 -> 28 x 28),\n        if in_channels != base_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                conv1x1(in_channels, base_channels * self.expansion, stride=2),  # stride = 2 due to make the size half\n                nn.BatchNorm2d(base_channels * self.expansion)\n            )\n            print(f\"self.shortcut for in_channels != channels * self.expansion: {self.shortcut}\")\n        else:\n            self.shortcut = nn.Sequential()\n            print(f\"self.shortcut for in_channels == channels * self.expansion: {self.shortcut}\")\n\n        print(\n            f\"##### in_channels:{in_channels}  base_channels:{base_channels} conv1 stride: {stride}, self.shortcut {self.shortcut}\")\n\n    def forward(self, x):\n        \"\"\"\n\n        Parameters\n        ----------\n        self\n        x\n\n        Returns\n        -------\n\n        \"\"\"\n        out = self.conv1(x)\n        print(f\"conv1(x).size(): {out.shape}\")\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += self.shortcut(x)\n        print(f\"self.shortcut(x).shape: {self.shortcut(x).shape}\")\n        print(f\"out += self.shortcut(x): {out.shape}\")\n        out = self.relu(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:22:35.206839Z","iopub.execute_input":"2023-02-15T03:22:35.207446Z","iopub.status.idle":"2023-02-15T03:22:35.227907Z","shell.execute_reply.started":"2023-02-15T03:22:35.207405Z","shell.execute_reply":"2023-02-15T03:22:35.226838Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    # at the last layer of each block, the channel is x4, for example resnet50 first Bottleneck,\n    # 1x1 conv 64 -> 3x3 conv 64 -> 1x1 conv 256\n    expansion = 4\n\n    def __init__(self, in_channels, base_channels, stride=1):\n        \"\"\"\n\n        Parameters\n        ----------\n        in_channels\n        base_channels\n        stride\n        \"\"\"\n        super().__init__()\n        self.conv1 = conv1x1(in_channels, base_channels)\n        self.bn1 = nn.BatchNorm2d(base_channels)\n        self.conv2 = conv3x3(base_channels, base_channels, stride)\n        self.bn2 = nn.BatchNorm2d(base_channels)\n        # third one has 4x more channels as out put\n        self.conv3 = conv1x1(base_channels, base_channels * self.expansion)\n        self.bn3 = nn.BatchNorm2d(base_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n\n        if in_channels != base_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                # need stride as parameter since conv2_x is stride=1, and others are 2\n                conv1x1(in_channels, base_channels * self.expansion, stride),\n                nn.BatchNorm2d(base_channels * self.expansion),\n            )\n            print(f\"self.shortcut for in_channels != channels * self.expansion: {self.shortcut}\")\n        else:\n            self.shortcut = nn.Sequential()\n            print(f\"self.shortcut for in_channels == channels * self.expansion: {self.shortcut}\")\n\n        print(\n            f\"##### in_channels:{in_channels}  base_channels:{base_channels} conv2 stride: {stride}, self.shortcut {self.shortcut}\")\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += self.shortcut(x)\n        print(f\"self.shortcut(x).shape: {self.shortcut(x).shape}\")\n        print(f\"out += self.shortcut(x): {out.shape}\")\n\n        out = self.relu(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:20:31.431747Z","iopub.execute_input":"2023-02-15T03:20:31.432137Z","iopub.status.idle":"2023-02-15T03:20:31.443190Z","shell.execute_reply.started":"2023-02-15T03:20:31.432106Z","shell.execute_reply":"2023-02-15T03:20:31.442162Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\nclass Resnet(nn.Module):\n    def __init__(self, block_type, layers: tuple, num_classes=1000):\n        \"\"\"\n\n        Parameters\n        ----------\n        block_type: BasicBlock, Bottleneck\n            Whether this resnet use BasicBlock (Res18/Res34) or Bottleneck(Res50 or higher)\n        layers: tuple\n            contains number of covolution in each layer, for example for Resnet50 (3,4,6,4)\n        num_classes: int\n            number of output classes\n        \"\"\"\n        super().__init__()\n\n        self.in_channels = 64  # initial channel for all type is 64\n\n        # initial conv1_x layer, number of input_channel = 3(R,G,B), number of output_channel = 64\n        # Input image size 224*224*3(RGB), padding = (3,3) to make size 230 * 230 ->  112*112 with stride (2,2)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        # conv2_x layer, input size is (batch_size, 64, 112, 112)\n        self.maxpool = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv2_x = self._create_layer(block_type, 64, layers[0], stride=1)  # for resnet50 layers[0] = 3\n        self.conv3_x = self._create_layer(block_type, 128, layers[1], stride=2)  # for resnet50 layers[1] = 4\n        self.conv4_x = self._create_layer(block_type, 256, layers[2], stride=2)  # for resnet50 layers[2] = 6\n        self.conv5_x = self._create_layer(block_type, 512, layers[3], stride=2)  # for resnet50 layers[3] = 3\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block_type.expansion, num_classes)\n\n    def _create_layer(self, block_type, base_channels, repeated_conv_blocks, stride):\n        \"\"\"\n        create layers, conv2_x, conv3_x, conv4_x and conv5_x in Resnet18, 34, 50, 101 and 152\n\n        Parameters\n        ----------\n        block_type:BasicBlock, Bottleneck\n            Whether this resnet use BasicBlock (Res18/Res34) or Bottleneck(Res50 or higher)\n        base_channels: int\n            for conv2_x: 64, conv3_x:128 , conv4_x = 256, conv5_x 512. These numbers are common across all Resnet type\n        repeated_conv_blocks : int\n            number of convolution blocks repeated, for example res50 conv2_x repeats 3 times and conv4_x repeats 6 times\n        stride: int or tuple(int,int)\n\n        Returns\n        -------\n            nn.Sequential\n        \"\"\"\n        layers = []\n\n        # first Residual basic block (or bottleneck), constructs basic block (or bottleneck) and appends to list\n        layers.append(block_type(self.in_channels, base_channels, stride))\n        # rest of Residual basic block\n        self.in_channels = base_channels * block_type.expansion\n        for _ in range(1, repeated_conv_blocks):\n            layers.append(block_type(self.in_channels, base_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.maxpool(x)\n\n        x = self.conv2_x(x)\n        x = self.conv3_x(x)\n        x = self.conv4_x(x)\n        x = self.conv5_x(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:21:12.267445Z","iopub.execute_input":"2023-02-15T03:21:12.268600Z","iopub.status.idle":"2023-02-15T03:21:12.297557Z","shell.execute_reply.started":"2023-02-15T03:21:12.268551Z","shell.execute_reply":"2023-02-15T03:21:12.294710Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Use pretrained one so comment out\n# def resnet18():\n#     return ResNet(BasicBlock, tc.res18_layers)\n# def resnet34():\n#     return ResNet(BasicBlock, tc.res34_layers)\n# def resnet50():\n#     return ResNet(Bottleneck, tc.res50_layers)\n# def resnet101():\n#     return Resnet(Bottleneck, tc.res101_layers)\n# def resnet152():\n#     return Resnet(Bottleneck, tc.re152_layers)","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:21:15.494780Z","iopub.execute_input":"2023-02-15T03:21:15.495494Z","iopub.status.idle":"2023-02-15T03:21:15.500447Z","shell.execute_reply.started":"2023-02-15T03:21:15.495458Z","shell.execute_reply":"2023-02-15T03:21:15.499232Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Resnet 18 (or 152) and change last layer output to 13\nmodel = resnet152(pretrained = True)\n# Change the last output to 13\nfc_in_features = model.fc.in_features\nmodel.fc = torch.nn.Linear(fc_in_features, (len(tc.classes)))\nmodel = model.to(tc.device)","metadata":{"id":"fRNx53rwRWSn","execution":{"iopub.status.busy":"2023-02-15T03:21:18.340033Z","iopub.execute_input":"2023-02-15T03:21:18.340438Z","iopub.status.idle":"2023-02-15T03:21:46.115532Z","shell.execute_reply.started":"2023-02-15T03:21:18.340407Z","shell.execute_reply":"2023-02-15T03:21:46.114531Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06959b6007204d9283f5a0767edc1fa9"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"7mYFpjLQRWSn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{"id":"mwFF7LkBRWSn"}},{"cell_type":"code","source":"## util functions\n\ndef get_mean_std(dataset):\n    \"\"\"returns mean and standard deviation of dataset given, since this method will take long time, once calcuated, not been used\"\"\"\n    # calculated mean: tensor([0.5768, 0.4622, 0.3460]), std:tensor([0.2699, 0.2739, 0.2826]):\n    loader = DataLoader(dataset)\n    \n    batch_mean = torch.zeros(3) # tensor([0,0,0])\n    batch_mean_sqrd = torch.zeros(3)\n    \n    for batch_data, _ in loader:\n        batch_mean += batch_data.mean(dim=(0,2,3))\n        batch_mean_sqrd += (batch_data **2).mean(dim=(0,2,3))\n        \n    mean = batch_mean / len(loader)\n    var = (batch_mean_sqrd) / len(loader) - (mean **2)\n    \n    std = var ** .5\n    \n    print(\"mean: {}, std:{}:\".format(mean, std))\n    return mean, std\n\n\ndef eval_loss(data_loader, device, model, criterion):\n    \"\"\"evaluate losses and can use this for visualize\"\"\"\n    for images, labales in data_loader:\n        break\n        \n    inputs = images.to(device)\n    labels = labels.to(device)\n    \n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    \n    return loss\n\ndef evaluate_history(history):\n    import matplotlib.pyplot as plt\n    print(\"initial stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[0,3], history[0,4]))\n    print(\"final stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[-1,3], history[-1,4]))\n    \n    num_epochs = len(history)\n    unit = num_epochs/10\n    \n    # Display learning curve (Loss)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,1], 'b', label='train')\n    plt.plot(history[:,0], history[:,3], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1, unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Loss)')\n    plt.legend()\n    plt.show()\n\n    # Display learning curve (Accuracy)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,2], 'b', label='train')\n    plt.plot(history[:,0], history[:,4], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1,unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Accuracy')\n    plt.legend()\n    plt.show()\n\n\n# model save and load functions\ndef save_model(model, device):\n    if not os.path.exists(tc.model_dir):\n        os.makedirs(tc.model_dir)\n    \n    model_path = os.path.join(tc.model_dir, tc.model_name)\n    \n    if device == \"cuda\":\n        model.to(\"cpu\")\n    \n    torch.save(model.state_dict(), model_path + \"best_model.pt\")\n    \n    if device == \"cuda\":\n        model.to(\"cuda\")\n    \n    return\n\ndef load_model(model):\n    \n    model_path = os.path.join(tc.model_dir, tc.model_name)\n    model.load_state_dict(torch.load(model_path + 'best_model.pt'))\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* id: 0 'githeri': 479,\n* id: 1 'ugali': 628,\n* id: 2 'kachumbari': 494,\n* id: 3 'matoke': 483,\n* id: 4 'sukumawiki': 402,\n* id: 5 'bhaji': 632,\n* id: 6 'mandazi': 620,\n* id: 7 'kukuchoma': 173,\n* id: 8 'nyamachoma': 784,\n* id: 9 'pilau': 329,\n* id:10 'chapati': 862,\n* id:11 'masalachips': 438,\n* id:13 'mukimo': 212","metadata":{}},{"cell_type":"code","source":"# Seems this doesn't work\n# from torch.utils.data.sampler import BatchSampler\n\n# class BalancedBatchSampler(BatchSampler):\n    \n    \n#     def __init__(self, dataset, n_classes, n_samples):\n#         loader = DataLoader(dataset)\n#         # keep ALL the label as list\n#         self.labels_list = []\n#         for _, label in loader:\n#             self.labels_list.append(label)\n#         # and keep it as Tensor\n#         self.labels = torch.LongTensor(self.labels_list)\n        \n#         self.labels_set = list(set(self.labels.numpy()))\n#         self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n#                                  for label in self.labels_set}\n#         for l in self.labels_set:\n#             np.random.shuffle(self.label_to_indices[l])\n#         self.used_label_indices_count = {label: 0 for label in self.labels_set}\n#         self.count = 0\n#         self.n_classes = n_classes\n#         self.n_samples = n_samples\n#         self.dataset = dataset\n#         self.batch_size = self.n_samples * self.n_classes\n        \n#     def __iter__(self):\n#         self.count = 0\n#         while self.count + self.batch_size < len(self.dataset):\n#             classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n#             indices = []\n#             for class_ in classes:\n#                 indices.extend(self.label_to_indices[class_][\n#                                self.used_label_indices_count[class_]:self.used_label_indices_count[\n#                                                                          class_] + self.n_samples])\n#                 self.used_label_indices_count[class_] += self.n_samples\n#                 if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n#                     np.random.shuffle(self.label_to_indices[class_])\n#                     self.used_label_indices_count[class_] = 0\n#             yield indices\n#             self.count += self.n_classes * self.n_samples\n\n#     def __len__(self):\n#         return len(self.dataset) // self.batch_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tools to use matplot to visualize data balanced each batch\ndef visualize_batch_data(dataloader, with_visual_chart = True , with_class_string = True) -> dict:\n    \"\"\"visualize what class is in the batch to visually see the equalness of the samples\"\"\"\n    #{0: 'githeri', 1: 'ugali', 2: 'kachumbari', 3: 'matoke', 4: 'sukumawiki', 5: 'bhaji', 6: 'mandazi', 7: 'kukuchoma', 8: 'nyamachoma', 9: 'pilau', 10: 'chapati', 11: 'masalachips', 12: 'mukimo'}\n    # total_number_appeared: Counter({10: 862, 8: 784, 5: 632, 1: 628, 6: 620, 2: 494, 3: 483, 0: 479, 11: 438, 4: 402, 9: 329, 12: 212, 7: 173})\n    total_num_images = len(data_loader.dataset)\n    total_number_appeared = collections.Counter()\n    number_appeared_per_batch = []\n    dict_classes = data_loader.dataset.dict_classes\n    \n    for index, (_, label_index) in enumerate(data_loader): # don't use image\n#         class_ids, class_counts = torch.unique(label_index, return_counts=True) # return in torch.tensor\n        number_appeared = collections.Counter(label_index.tolist())\n        print(f\"batch:{index} number_appeared:{number_appeared}\")\n        \n        # add zero for sample which was not picked, and add that label_index:0 to Counter\n        while len(number_appeared) < len(dict_classes):\n            for i in range(len(number_appeared)):\n                number_appeared.setdefault(i,0)\n            print(f\"batch:{index} number_appeared_after_setdefault:{number_appeared}\")\n        # for total count\n        total_number_appeared += number_appeared\n\n            \n        # sort based on keys (label_index) and cast to dict for visualization\n        dict_number_appeared = dict(sorted(number_appeared.items()))\n        #print(f\"dict_number_appeared:{dict_number_appeared}\")\n        number_appeared_per_batch.append(dict_number_appeared)\n        \n    if with_visual_chart:\n        \"\"\"create bar graph for all batches which class had samples\"\"\"\n        fig, axes = plt.subplots(int(tc.batch_size/4), 4 , figsize = (20, 20))\n        for i in range(int(tc.batch_size/4)):\n            for j in range(4):\n                axes[i][j].bar(number_appeared_per_batch[int(j+(i*4))].keys(), list(number_appeared_per_batch[int(j+(i*4))].values()))\n                if with_class_string:\n                    axes[i][j].set_xticks(list(range(len(dict_classes))))\n                    axes[i][j].set_xticklabels(list(dict_classes.values()))\n                else:\n                    axes[i][j].set_xticks(list(range(len(dict_classes))))\n                    axes[i][j].set_xticklabels(list(dict_classes.keys()))\n\n    \n    # return dict type, key=class name, value=how many times appeared\n    if with_class_string:\n        return {data_loader.dataset.index2class(key):value for key, value in total_number_appeared.items()}\n    \n    # return dict type, key=class_id, value=how many times appeared\n    return dict(total_number_appeared)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{"id":"tyZ0aRAaRWSn"}},{"cell_type":"code","source":"train_dataset = KenyanFood13SplitDataset(transform = train_preprocess(),train = True)\nvalidation_dataset = KenyanFood13SplitDataset(transform = validation_preprocess(), train = False)\nsequential_sampler = torch.utils.data.SequentialSampler(train_dataset)\nweight_sampler = WeightedRandomSampler(weights=train_dataset.samples_weights, num_samples=len(train_dataset), replacement=True)\ntrain_loader = DataLoader(train_dataset, sampler = weight_sampler, batch_size = tc.batch_size, shuffle = False)\nvalidation_loader = DataLoader(validation_dataset, batch_size = tc.batch_size, shuffle = False )\n# create optimizer\noptimizer = SGD(model.parameters(), lr=tc.lr, momentum = 0.9)\nprint(f\"length of train_dataset {len(train_dataset.samples_weights)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_val_loss = np.Inf\nhistory = np.zeros((0,6))\nfor epoch in range(tc.epoch_count):\n    # time one cycle of train and validate\n    start_time = time.time()\n    train_loss, train_acc = train(model, train_loader, optimizer)\n    val_loss, val_acc = validate(model, validation_loader)\n    elapsed_time = time.time() - start_time\n    \n    # save the weight when the loss gets smaller than last one\n    if val_loss < best_val_loss:\n        save_model(model, tc.device)\n        print(\"WEIGHTS-ARE-SAVED\")\n        best_val_loss = val_loss\n        \n    print(f\"Epoch{epoch+1}/{tc.epoch_count}, loss:{train_loss:.5f}, acc:{train_acc:.5f}, val_loss:{val_loss:.5f}, val_acc:{val_acc:.5f}, elapsed_time:{elapsed_time:.5f}\")\n    # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n    item = np.array([epoch+1, train_loss, train_acc, val_loss, val_acc, elapsed_time])\n    # store history for matplotlib visualization\n    history = np.vstack((history, item))\n\n    # For tensorboard\n    tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_loss, epoch+1)\n    tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_acc, epoch+1)\n    tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now do the inference and get submission.csv file\ntest_dataset = KenyanFood13Testset()\ntest_loader = DataLoader(test_dataset, batch_size = tc.batch_size, shuffle = False)\n\n# prediction\npredicted_result = predict(model, test_loader)\n#print(\"predicted_result len:{}\".format(len(predicted_result)))\nclassifications = []\n\n#change predicted index to class_names\nfor index in range(len(predicted_result)):\n    predicted_result_class = test_dataset.base_dataset.index2class(int(predicted_result[index]))\n    classifications.append(predicted_result_class)\n\n# put it to csv file\nclasses = pd.DataFrame(classifications, columns = [\"class\"])\nresult = test_dataset.test_labels.join(classes)\nresult.to_csv(tc.submission_csv, index=False, header = True)","metadata":{"id":"WqFnOpy1RWSn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"f7bNvOcZRWSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dont use anymore... refactored\n# def fit(model, optimizer, train_loader, validation_loader, history):\n#     \"\"\"works like tensorflow fits. epoch_count, optimizer and criterion(e.g., CrossEntorpyLoss) from training config\"\"\"\n    \n#     #from tqdm.notebook import tqdm\n    \n#     base_epochs = len(history)\n    \n#     for epoch in range(base_epochs, tc.epoch_count + base_epochs):\n#         train_loss = 0\n#         train_acc = 0\n#         val_loss = 0\n#         val_acc = 0\n#         best_valid_loss = np.Inf\n        \n#         # training Phase\n#         model.train()\n#         count = 0\n        \n#         t_begin = time.time()\n#         for inputs, labels in train_loader:\n            \n#             count += len(labels)\n#             inputs = inputs.to(tc.device)\n#             labels = labels.to(tc.device)\n            \n#             # initialize the gradients\n#             optimizer.zero_grad()\n#             # predict\n#             outputs = model(inputs)\n#             # calculate loss\n#             loss = tc.criterion(outputs, labels)\n#             train_loss += loss.item()\n            \n#             #calucalte the graidents\n#             loss.backward()\n#             #update parameter\n#             optimizer.step()\n            \n#             # get highest predicted value\n#             predicted = torch.max(outputs, axis=1)[1]\n#             # calculate number of correct answer\n#             train_acc  += (predicted == labels).sum().item()\n            \n#             #calculate the average loss and accuracy in the batch\n#             avg_train_loss = train_loss / count\n#             avg_train_acc = train_acc / count\n            \n#         #evaluation phase\n#         model.eval()\n#         count = 0\n        \n#         for inputs, labels in validation_loader:\n#             count += len(labels)\n#             inputs = inputs.to(tc.device)\n#             labels = labels.to(tc.device)\n            \n#             # calculate the prediction\n#             outputs = model(inputs)\n            \n#             # calculate the loss\n#             loss = tc.criterion(outputs, labels)\n#             val_loss += loss.item()\n            \n#             if val_loss < best_valid_loss:\n#                 torch.save(model.state_dict(), \"best_model.pt\")\n#                 print(\"WEIGHTS-ARE-SAVED\")\n#                 best_valid_loss = val_loss\n                \n#             # get highest predicted value\n#             predicted = torch.max(outputs, axis=1)[1]\n#             # calculate number of correct answer\n#             val_acc += (predicted == labels).sum().item()\n            \n#             #calculate the average loss and accuracy in the batch\n#             avg_val_loss = val_loss / count\n#             avg_val_acc = val_acc / count\n            \n            \n#         elapsed_time = time.time() - t_begin\n#         # print out\n#         print(\"Epoch{}/{},  loss:{:.5f}, acc:{:.5f}, val_loss:{:.5f}, val_acc:{:.5f}, elapsed_time:{:.5f}\".format(\n#             epoch+1,tc.epoch_count+base_epochs, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time\n#         ))\n#         # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n#         item = np.array([epoch+1, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time])\n#         history = np.vstack((history, item))\n        \n#         # For tensorboard\n#         tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_loss, epoch+1)\n#         tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_acc, epoch+1)\n#         tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)\n\n        \n#     return history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n\n**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\nFor example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars).","metadata":{"id":"QRbjuol1RWSo"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"https://github.com/caq05630","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"NHuqwjtrRWSo"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{"id":"zP0rAnxrRWSo"}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/yasuokada/project2/edit/run/117796209","metadata":{}}]}