{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{"id":"rVHwWejfRWSd"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n    \n```\n\n```\ndef get_data(args1, *agrs):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{"id":"EI9ivVwbRWSi"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**class_counts: (very uneven)**\n1. chapati        862\n1. nyamachoma     784\n1. bhaji          632\n1. ugali          628\n1. mandazi        620\n1. kachumbari     494\n1. matoke         483\n1. githeri        479\n1. masalachips    438\n1. sukumawiki     402\n1. pilau          329\n1. mukimo         212\n1. kukuchoma      173\n\nName: class, dtype: int64, sum: 6536\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:12.679939Z","iopub.execute_input":"2023-01-30T03:32:12.680319Z","iopub.status.idle":"2023-01-30T03:32:12.694310Z","shell.execute_reply.started":"2023-01-30T03:32:12.680287Z","shell.execute_reply":"2023-01-30T03:32:12.693166Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Import neccesaary libraries\nimport os\nimport time\nimport collections\nfrom dataclasses import dataclass\n\n# third party library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n\n# Pytorch related\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import WeightedRandomSampler\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Subset\nfrom torch.optim import SGD  \nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom torchvision.models import resnet152, resnet18","metadata":{"id":"xlzaxohURWSi","executionInfo":{"status":"ok","timestamp":1672696976022,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-01-30T03:32:12.719278Z","iopub.execute_input":"2023-01-30T03:32:12.719553Z","iopub.status.idle":"2023-01-30T03:32:13.778757Z","shell.execute_reply.started":"2023-01-30T03:32:12.719528Z","shell.execute_reply":"2023-01-30T03:32:13.777802Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# all the Transforms\n# def resize_preprocess():\n#     \"\"\"Compulsory transforms image to same_size and center cropped (not changing to Tensor yet)\"\"\"\n#     resize_preprocess = transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#     ])\n    \n#     return resize_preprocess\n\n\ndef small_image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset holding original database(No need to hold big image)\"\"\"\n    small_image_preprocess = transforms.Compose([\n        transforms.Resize(128),\n        transforms.CenterCrop(112),\n        transforms.ToTensor()\n    ])\n    \n    return small_image_preprocess\n\n### from here to below will change to Tensor\ndef image_preprocess_transforms():\n    \"\"\"pre_process for KenyanFood13Testset \"\"\"\n    image_preprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor()\n    ])\n    \n    return image_preprocess\n\n\n\ndef train_preprocess():\n    \"\"\"resize_preprocess() + couple transformation to improve accuracy in training, ToTensor and Normalization\"\"\"\n    transforms_train = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.RandomHorizontalFlip(p=0.5),\n        # Somehow this doesn't work\n        #transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio = (0.33, 0.33), value= 0, inplace = False), \n        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n        transforms.RandomRotation(degrees=10),\n        transforms.RandomPosterize(bits=2),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)        \n\n\n\n    ])\n    return transforms_train\n    \ndef validation_preprocess():\n    \"\"\" image_preprocess() + Normalization\"\"\"\n    validation_train = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(tc.mean, tc.std)\n    ])\n    return validation_train\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:13.782477Z","iopub.execute_input":"2023-01-30T03:32:13.783317Z","iopub.status.idle":"2023-01-30T03:32:13.792915Z","shell.execute_reply.started":"2023-01-30T03:32:13.783286Z","shell.execute_reply":"2023-01-30T03:32:13.791999Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Datasets Classes\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"create KenyanFood Dataset from annotations_file and images\"\"\"\n    def __init__(self, transform = None):\n        self.img_labels = pd.read_csv(tc.annotation_file)\n        self.img_dir = tc.img_dir\n        # Make list of the classes list will give index (iterable) -> 13 classes\n        self.classes = list(self.img_labels['class'].unique()) \n        # Make dictionary set of the classes \n        self.dict_classes = dict(enumerate(self.img_labels['class'].unique()))\n        self.transform = transform\n\n        def _inverse_ratio():\n            \"\"\"calculate inverse ratio of number of each classes for weighted sampler usage\"\"\"\n            series = self.img_labels['class'].value_counts()\n            total = series.sum()\n            for index, _ in enumerate(series):\n                series.iloc[index] = series.iloc[index]/total\n            return series\n        \n        # weigths of each class normalized (total = 1.0)\n        self.weights = _inverse_ratio()\n\n        \n        \n    def __len__(self):\n        return len(self.img_labels)\n    \n    def __getitem__(self, index):\n        \"\"\"Return (image, target) after resizing and preprocessing \n        iloc[index, 0] will return ids (e.g.,14278962987112149800) of each row\"\"\"\n        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[index, 0])+\".jpg\")\n        image = Image.open(img_path)\n                \n        # label is string so will return index (pytorch cannot make string to tensor)\n        # iloc[index, 1] will return the class_name (e.g., githeri) for id in [index, 1] (e.g., 14278962987112149800)\n        label_index = self.class2index(self.img_labels.iloc[index, 1])  # returns int\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label_index #image: Image or Tensor ,  label_index: int\n    \n    \n    def class2index(self, class_name:str)-> int:\n        \"\"\"Returns the index of a given class.\"\"\"\n        return self.classes.index(class_name)\n    \n    \n    def index2class(self, class_index:int)-> str:\n        \"\"\"Returns the class of a given index.\"\"\"\n        return self.classes[class_index]\n\n    \nclass KenyanFood13SplitDataset(Dataset):\n    \"\"\"creates split train and validation subset from KenyanFood13Dataset based on indices\"\"\"\n    def __init__(self, indices:np, transform = None):\n        self.indices = indices\n        self.transform = transform\n        self.dataset = KenyanFood13Dataset()\n        \n    def __getitem__(self, index):\n        image, label = self.dataset[self.indices[index]]\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n    \n    def __len__(self):\n        return len(self.indices)\n\n    \nclass KenyanFood13Testset(Dataset):\n    \"\"\"Kenyan food test dataset, contains original KenanFood13Dataset for image2class and class2image methods\"\"\"\n    def __init__(self, transform = image_preprocess_transforms()):\n        self.img_dir = tc.img_dir\n        self.test_labels = pd.read_csv(tc.test_csv_file)\n        self.transform = transform\n        # this is to use class2index() and class2index()\n        self.base_dataset = KenyanFood13Dataset(small_image_preprocess_transforms())\n\n    def __getitem__(self, index):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        \n        # get img path from test_labels. img_dir + id in csv file (per index) + \".jpg\" \n        img = os.path.join(self.img_dir, str(self.test_labels.iloc[index, 0]) + '.jpg')\n        \n        image = Image.open(img)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n    \n    def __len__(self):\n        return len(self.test_labels)","metadata":{"id":"wmEnYe40RWSj","executionInfo":{"status":"ok","timestamp":1672695586248,"user_tz":-540,"elapsed":237,"user":{"displayName":"Yasu OKADA","userId":"02241740618725882078"}},"execution":{"iopub.status.busy":"2023-01-30T03:32:13.794439Z","iopub.execute_input":"2023-01-30T03:32:13.795407Z","iopub.status.idle":"2023-01-30T03:32:13.812481Z","shell.execute_reply.started":"2023-01-30T03:32:13.795370Z","shell.execute_reply":"2023-01-30T03:32:13.811543Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{"id":"oG7W87aVRWSk"}},{"cell_type":"code","source":"# Settings and configurations\n@dataclass\nclass SystemConfiguration:\n    seed:int = 21\n        \n# Specifiy all the data needed in dataclass named TrainingConfiguration\n@dataclass\nclass TrainingConfiguration:\n    batch_size: int = 20\n    epoch_count: int = 5\n    init_learning_rate: float = 0.001\n    log_interval: int = 5\n    test_interval: int = 1\n    data_root: str = \"/kaggle/input/opencv-pytorch-dl-course-classification/\"\n    model_dir: str = '/kaggle/working/models/'\n    log_dir: str = '/kaggle/working/logs'\n    annotation_file: str = data_root + \"train.csv\"\n    test_csv_file: str = data_root + \"test.csv\"\n    img_dir: str = data_root + \"images/images/\"\n    submission_csv: str = '/kaggle/working/submission.csv'\n    num_workers: int = 2\n    device: str = 'cuda'\n    train_split: float = 0.8\n    random_seed: int = 42\n    model_name: str = \"resnet152\"\n    lr: float =0.00001\n    classes: tuple = ('githeri', 'ugali', 'kachumbari', 'matoke', 'sukumawiki', 'bhaji', 'mandazi', 'kukuchoma', 'nyamachoma', 'pilau', 'chapati', 'masalachips', 'mukimo')\n    dataset_len: int = 6536\n    #below is calculated from KenyanFood13 pictures\n    #mean: torch.tensor = torch.tensor([0.5768, 0.4622, 0.3460])\n    #std: torch.tensor = torch.tensor([0.2699, 0.2739, 0.2826])\n    \n    #below mean and standard are for Resnet18 https://pytorch.org/hub/pytorch_vision_resnet/\n    mean: torch.tensor = torch.tensor([0.485, 0.456, 0.406])\n    std: torch.tensor  = torch.tensor([0.229, 0.224, 0.225])\n        \n    tb_writer: SummaryWriter = SummaryWriter(log_dir)\n    criterion: torch.nn.CrossEntropyLoss = torch.nn.CrossEntropyLoss()        \n\ndef setup_system(SystemConfiguration):\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.backends.cudnn_benchmark_enabled = True\n        torch.backends.cudnn.deterministic = True\n        \n        \ntc = TrainingConfiguration()\nsystem_config = SystemConfiguration()\nsetup_system(system_config)\n\n\nfor path in [tc.log_dir, tc.model_dir]:\n    if not os.path.exists(path):\n        os.makedirs(path)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:13.816177Z","iopub.execute_input":"2023-01-30T03:32:13.817294Z","iopub.status.idle":"2023-01-30T03:32:18.024213Z","shell.execute_reply.started":"2023-01-30T03:32:13.817258Z","shell.execute_reply":"2023-01-30T03:32:18.023233Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"372d290ERWSl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{"id":"XtEdfRZjRWSl"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{"id":"qqXcmrqbRWSm"}},{"cell_type":"code","source":"def train(model, train_loader, optimizer) -> float:\n    \"\"\"train by model by given train_loader(and its datasets) and optimizer, return average loss and accuracy\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.train()\n    \n    for inputs, labels in tqdm(train_loader):\n        count += len(labels)\n        # move to target device (GPU or CPU)\n        inputs = inputs.to(tc.device)\n        labels = labels.to(tc.device)\n        \n        # prediction\n        outputs = model(inputs)\n        # calculate loss\n        loss = tc.criterion(outputs, labels)\n        # initialize the gradients -> calucalte the gradients -> Update the gradients\n        optimizer.zero_grad()     \n        loss.backward()\n        optimizer.step()\n        \n        # adds up loss pf number of batches\n        total_loss += loss.item()\n        # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n        avg_loss = total_loss / count\n        \n        # get highest predicted value\n        predicted = torch.max(outputs, axis=1)[1]\n        # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n        total_acc  += (predicted == labels).sum().item()\n        avg_acc =  total_acc / count\n        \n    return avg_loss, avg_acc","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:18.025508Z","iopub.execute_input":"2023-01-30T03:32:18.025873Z","iopub.status.idle":"2023-01-30T03:32:18.034008Z","shell.execute_reply.started":"2023-01-30T03:32:18.025835Z","shell.execute_reply":"2023-01-30T03:32:18.032845Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def validate(model, validation_loader):\n    \"\"\"validate by model by given validation_loader(and its datasets) , return average loss\"\"\"\n    total_loss = 0.0\n    total_acc = 0\n    count = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(validation_loader):\n            count += len(labels)\n            # move to target device (GPU or CPU)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # prediction\n            outputs = model(inputs)\n            # calculate loss\n            loss = tc.criterion(outputs, labels)\n            \n            # adds up loss pf number of batches\n            total_loss += loss.item()\n            # calculates accumulative average loss by dividing total loss by length of dataset predicted so far\n            avg_loss = total_loss / count\n            \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            total_acc  += (predicted == labels).sum().item()\n            # calculates accumulative average accuracy by dividing total loss by length of dataset predicted so far\n            avg_acc =  total_acc / count\n        \n            \n    return avg_loss, avg_acc\n        ","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:18.035731Z","iopub.execute_input":"2023-01-30T03:32:18.036499Z","iopub.status.idle":"2023-01-30T03:32:18.051293Z","shell.execute_reply.started":"2023-01-30T03:32:18.036463Z","shell.execute_reply":"2023-01-30T03:32:18.050392Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def predict(trained_model, prediction_loader) -> list:\n    \"\"\"prediction with model and test_loader, returns list of prediction result, assume to be used for inference ONLY\"\"\"  \n    predictions = torch.tensor([]).to(tc.device)\n    for inputs in prediction_loader:\n        inputs = inputs.to(tc.device)\n        outputs = trained_model(inputs)\n        # get the predicted result by index number, each prediction will be done by tc.batch_size so predicted is array of batch_size\n        predictions = torch.hstack((predictions, torch.max(outputs, axis=1)[1]))\n        \n    return predictions.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:18.053722Z","iopub.execute_input":"2023-01-30T03:32:18.054044Z","iopub.status.idle":"2023-01-30T03:32:18.067583Z","shell.execute_reply.started":"2023-01-30T03:32:18.054019Z","shell.execute_reply":"2023-01-30T03:32:18.066743Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{"id":"0-ysifviRWSm"}},{"cell_type":"code","source":"# Resnet 18 (or 152) and change last layer output to 13\nmodel = resnet152(pretrained = True)\n# Change the last output to 13\nfc_in_features = model.fc.in_features\nmodel.fc = torch.nn.Linear(fc_in_features, (len(tc.classes)))\nmodel = model.to(tc.device)","metadata":{"id":"fRNx53rwRWSn","execution":{"iopub.status.busy":"2023-01-30T03:32:18.069126Z","iopub.execute_input":"2023-01-30T03:32:18.069537Z","iopub.status.idle":"2023-01-30T03:32:35.573276Z","shell.execute_reply.started":"2023-01-30T03:32:18.069500Z","shell.execute_reply":"2023-01-30T03:32:35.572270Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d254ae99504f53be156716b0ae0e96"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"7mYFpjLQRWSn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{"id":"mwFF7LkBRWSn"}},{"cell_type":"code","source":"## util functions\n\ndef get_mean_std(dataset):\n    \"\"\"returns mean and standard deviation of dataset given, since this method will take long time, once calcuated, not been used\"\"\"\n    # calculated mean: tensor([0.5768, 0.4622, 0.3460]), std:tensor([0.2699, 0.2739, 0.2826]):\n    loader = DataLoader(dataset)\n    \n    batch_mean = torch.zeros(3) # tensor([0,0,0])\n    batch_mean_sqrd = torch.zeros(3)\n    \n    for batch_data, _ in loader:\n        batch_mean += batch_data.mean(dim=(0,2,3))\n        batch_mean_sqrd += (batch_data **2).mean(dim=(0,2,3))\n        \n    mean = batch_mean / len(loader)\n    var = (batch_mean_sqrd) / len(loader) - (mean **2)\n    \n    std = var ** .5\n    \n    print(\"mean: {}, std:{}:\".format(mean, std))\n    return mean, std\n\n\ndef eval_loss(data_loader, device, model, criterion):\n    \"\"\"evaluate losses and can use this for visualize\"\"\"\n    for images, labales in data_loader:\n        break\n        \n    inputs = images.to(device)\n    labels = labels.to(device)\n    \n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    \n    return loss\n\ndef evaluate_history(history):\n    import matplotlib.pyplot as plt\n    print(\"initial stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[0,3], history[0,4]))\n    print(\"final stats: Loss{:.5f}  Accuracy{:.5f}\".format(history[-1,3], history[-1,4]))\n    \n    num_epochs = len(history)\n    unit = num_epochs/10\n    \n    # Display learning curve (Loss)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,1], 'b', label='train')\n    plt.plot(history[:,0], history[:,3], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1, unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Loss)')\n    plt.legend()\n    plt.show()\n\n    # Display learning curve (Accuracy)\n    plt.figure(figsize=(9,8))\n    plt.plot(history[:,0], history[:,2], 'b', label='train')\n    plt.plot(history[:,0], history[:,4], 'k', label='validation')\n    plt.xticks(np.arange(0,num_epochs+1,unit))\n    plt.xlabel('# of iteration')\n    plt.ylabel('loss')\n    plt.title('learning curve (Accuracy')\n    plt.legend()\n    plt.show()\n\n\n# model save and load functions\ndef save_model(model, device):\n    if not os.path.exists(tc.model_dir):\n        os.makedirs(tc.model_dir)\n    \n    model_path = os.path.join(tc.model_dir, tc.model_name)\n    \n    if device == \"cuda\":\n        model.to(\"cpu\")\n    \n    torch.save(model.state_dict(), model_path + \"best_model.pt\")\n    \n    if device == \"cuda\":\n        model.to(\"cuda\")\n    \n    return\n\ndef load_model(model):\n    \n    model_path = os.path.join(tc.model_dir, tc.model_name)\n    model.load_state_dict(torch.load(model_path + 'best_model.pt'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:35.574742Z","iopub.execute_input":"2023-01-30T03:32:35.575344Z","iopub.status.idle":"2023-01-30T03:32:35.591955Z","shell.execute_reply.started":"2023-01-30T03:32:35.575305Z","shell.execute_reply":"2023-01-30T03:32:35.590960Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch\nb = torch.tensor([1,3,2,3,4,4,4,1,1,1,1,1,1])\nclass_ids, class_counts = torch.unique(b, return_counts=True)\nprint(f\"class_ids:{class_ids}\")\nprint(f\"class_counts:{class_counts}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:35.596184Z","iopub.execute_input":"2023-01-30T03:32:35.596463Z","iopub.status.idle":"2023-01-30T03:32:35.613304Z","shell.execute_reply.started":"2023-01-30T03:32:35.596438Z","shell.execute_reply":"2023-01-30T03:32:35.611841Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"class_ids:tensor([1, 2, 3, 4])\nclass_counts:tensor([7, 1, 2, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"kenyan_dataset = KenyanFood13Dataset(transform = image_preprocess_transforms()) \ndata_loader = DataLoader(kenyan_dataset, batch_size = tc.batch_size, shuffle = False)\na= range(0,len(data_loader.dataset.classes))\nprint(range(13))\nprint(a)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:32:35.614903Z","iopub.execute_input":"2023-01-30T03:32:35.615289Z","iopub.status.idle":"2023-01-30T03:32:35.647156Z","shell.execute_reply.started":"2023-01-30T03:32:35.615253Z","shell.execute_reply":"2023-01-30T03:32:35.646143Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"range(0, 13)\nrange(0, 13)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"1. chapati 862\n1. nyamachoma 784\n1. bhaji 632\n1. ugali 628\n1. mandazi 620\n1. kachumbari 494\n1. matoke 483\n1. githeri 479\n1. masalachips 438\n1. sukumawiki 402\n1. pilau 329\n1. mukimo 212\n1. kukuchoma 173","metadata":{}},{"cell_type":"code","source":"def visualize_batch_data(dataloader, with_visual_chart = True):\n    #{0: 'githeri', 1: 'ugali', 2: 'kachumbari', 3: 'matoke', 4: 'sukumawiki', 5: 'bhaji', 6: 'mandazi', 7: 'kukuchoma', 8: 'nyamachoma', 9: 'pilau', 10: 'chapati', 11: 'masalachips', 12: 'mukimo'}\n    # total_number_appeared: Counter({10: 862, 8: 784, 5: 632, 1: 628, 6: 620, 2: 494, 3: 483, 0: 479, 11: 438, 4: 402, 9: 329, 12: 212, 7: 173})\n    total_num_images = len(data_loader.dataset)\n    total_number_appeared = collections.Counter()\n    index_seen = []\n    \n    for index, (image, label_index) in enumerate(data_loader): # don't use image\n        class_ids, class_counts = torch.unique(label_index, return_counts=True) # return in torch.tensor\n        number_appeared = collections.Counter(label_index.tolist())\n        total_number_appeared += number_appeared\n        \n#         if with_visual_chart:\n#             plt.figure(figsize =(20,10))\n#             plt.bar(list(number_appeared.keys()), list(number_appeared.values()))\n#             plt.title(f'Batch:{index}')\n#             plt.show()\n\n    if with_visual_chart:\n        plt.figure(figsize = (20, 10))\n        plt.bar(list(total_number_appeared.keys()), list(total_number_appeared.values()),tick_label=list(data_loader.dataset.dict_classes.values()))\n        plt.show()\n        \n    return total_number_appeared\n\n#     fig, ax = plt.subplots(1, figsize = (15,15))\n#     ind = np.arange(len(class_0_batch_counts))\n#     width = 0.35\n\n#     ax.bar(ind, chapati_batch_counts, width, label =\"chapati\")\n#     ax.bar(ind, kukuchoma_batch_counts, width, label =\"kukuchoma\")\n\n#     ax.set_xticks(ind, ind + 1)\n#     ax.set_xlabel(\"Batch index\", fontsize = 12)\n#     ax.set_ylabel(\"Num of image in batch\", fontize = 12)\n#     ax.set_aspect(\"equal\")\n\n#     plt.legend()\n#     plt.show()\n\n#     num_images_seen = len(idx_seen)\n\n#     print(f'Avg Proportion of chapati per batch: {(np.array(chapati_batch_counts /tc.batch_size)).mean()}')\n#     print(f'Avg Proportion of kukuchoma per batch: {(np.array(kukuchoma_batch_counts /tc.batch_size)).mean()}')   \n\n#     print(\"===========================\")\n#     print(f\"num. unique image seen: {len(set(idxs_seen))/(total_num_images)}\")\n    \n#     return chapati_batch_counts, kukuchoma_batch_counts, idxs_seen","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:54:57.247434Z","iopub.execute_input":"2023-01-30T03:54:57.247796Z","iopub.status.idle":"2023-01-30T03:54:57.256207Z","shell.execute_reply.started":"2023-01-30T03:54:57.247765Z","shell.execute_reply":"2023-01-30T03:54:57.255166Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"kenyan_dataset = KenyanFood13Dataset(transform = image_preprocess_transforms()) \ndata_loader = DataLoader(kenyan_dataset, batch_size = tc.batch_size, shuffle = False)\nvisualize_batch_data(data_loader)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T03:55:00.850355Z","iopub.execute_input":"2023-01-30T03:55:00.850731Z","iopub.status.idle":"2023-01-30T03:57:43.229061Z","shell.execute_reply.started":"2023-01-30T03:55:00.850698Z","shell.execute_reply":"2023-01-30T03:57:43.228147Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1440x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Counter({0: 479,\n         1: 628,\n         2: 494,\n         3: 483,\n         4: 402,\n         5: 632,\n         6: 620,\n         7: 173,\n         8: 784,\n         9: 329,\n         10: 862,\n         11: 438,\n         12: 212})"},"metadata":{}}]},{"cell_type":"raw","source":"","metadata":{"execution":{"iopub.status.busy":"2023-01-29T20:26:53.557575Z","iopub.execute_input":"2023-01-29T20:26:53.557940Z","iopub.status.idle":"2023-01-29T20:26:59.187244Z","shell.execute_reply.started":"2023-01-29T20:26:53.557912Z","shell.execute_reply":"2023-01-29T20:26:59.185908Z"}}},{"cell_type":"code","source":"#array = np.array(['a', 'a', 'd', 'b', 'b', 'c'])\n#array = np.array(['a', 'a', 'd', 'b', 'b', 'c'])\narray = np.array([1, 1])\nclass_0_batch_counts = []\nclass_1_batch_counts = []\nclass_names, class_counts = np.unique(array, return_counts = True)\nprint(f\"class_names:{class_names}\")\nprint(f\"class_counts:{class_counts}\")\n\nif len(class_names) == 2: #both class appeared in this batch\n        class_0_batch_counts.append(class_counts[0])\n        class_1_batch_counts.append(class_counts[1])\n        print(\"I am in 1\")\n\nelif len(class_names) == 1 and 0 in class_names: # either one appeard and \n    class_0_batch_counts.append(class_counts[0])\n    class_1_batch_counts.append(0)\n    print(\"I am in 2\")\nelif len(class_names) == 1 and 1 in class_names:\n    class_0_batch_counts.append(0)\n    class_1_batch_counts.append(class_counts[0])\n    print(\"I am in 3\")\nelse:\n    raise ValueError(\"More than two classes detected\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kenyan_dataset = KenyanFood13Dataset(transform = image_preprocess_transforms()) \nprint(kenyan_dataset.dict_classes)\ndata_loader = DataLoader(kenyan_dataset, batch_size = tc.batch_size, shuffle = False)\n    \n# chapati,  kukuchoma, idxs_seen = visualzie_dataloader(data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{"id":"tyZ0aRAaRWSn"}},{"cell_type":"code","source":"kenyan_dataset = KenyanFood13Dataset(transform = image_preprocess_transforms()) \nkenyan_dataset.classes\n#kenyan_dataset.dict_classes","metadata":{"id":"NU35cGsZRWSn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KenyanFood13Datase","metadata":{}},{"cell_type":"code","source":"kenyan_dataset = KenyanFood13Dataset(transform = image_preprocess_transforms()) \ntrain_loader = DataLoader(kenyan_dataset, batch_size = tc.batch_size, shuffle = False)\n\nfor i, (imageTensor, label_index) in enumerate(train_loader):\n    print(label_index)\n    class_list = [kenyan_dataset.index2class(label_index[i]) for i in label_index]\n    print(class_list)\n    #print(type(batch[1]))\n    if i == 1:\n        break\n\nfor i, (imageTensor, label_index) in enumerate(train_loader):\n    print(label_index)\n    class_list = [kenyan_dataset.index2class(label_index[i]) for i in label_index]\n    print(class_list)\n    #print(type(batch[1]))\n    if i == 1:\n        break\n        \n\nfor i, batch in enumerate(train_loader):\n    print(label_index[1])\n    #class_list = [kenyan_dataset.index2class(label_index[i]) for i in label_index]\n    #print(class_list)\n    #print(type(batch[1]))\n    if i == 1:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split dataset into train & validation dataset based on split ratio\nkenyan_dataset = KenyanFood13Dataset() \ntrain_size = int(len(kenyan_dataset) * tc.train_split)\nindices = np.arange(len(kenyan_dataset))\ntrain_dataset = KenyanFood13SplitDataset(indices[:train_size], train_preprocess())\nvalidation_dataset = KenyanFood13SplitDataset(indices[train_size:], validation_preprocess())\n\n# create train & validation loader\ntrain_sampler = WeightedRandomSampler(weights= kenyan_dataset.weights, num_samples=len(train_dataset), replacement=True)\ntrain_loader = DataLoader(train_dataset, sampler= train_sampler, batch_size = tc.batch_size, shuffle = False)\nvalidation_sampler = WeightedRandomSampler(weights= kenyan_dataset.weights, num_samples=len(validation_dataset), replacement=True)\nvalidation_loader = DataLoader(validation_dataset, sampler= validation_sampler,batch_size = tc.batch_size, shuffle = False)\n\n# create optimizer\noptimizer = SGD(model.parameters(), lr=tc.lr, momentum = 0.9)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kenyan_dataset = KenyanFood13Dataset(transform = image_preprocess_transforms())\ntrain_loader = DataLoader(kenyan_dataset, batch_size = tc.batch_size, shuffle=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame = pd.read_csv(tc.annotation_file)\nclass_counts = data_frame['class'].value_counts()\nclass_weights = 1/(class_counts * class_weights.sum())\n#total = series.sum()\nprint(f'class_counts: \\n {class_counts}, sum: {class_counts.sum()}')\nprint(f'class_weights: \\n {class_weights}, sum: {class_weights.sum()}')\n    \n# for index, value in enumerate(series):\n#     series.iloc[index] = series.iloc[index]/total\n\n# print(f'series: \\n{series}')\n# print(f'series sum:{series.sum()}')\n\n# sample_weights = [1/series.values for i in series]\n# print(sample_weights)\n#print(class_counts.iat[0])\n\n# for i in class_counts:\n#     print(i)\n# samples_weights =  [1/(class_counts[i] * class_weights.sum()) for i in class_counts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_val_loss = np.Inf\nhistory = np.zeros((0,6))\nfor epoch in range(tc.epoch_count):\n    # time one cycle of train and validate\n    start_time = time.time()\n    train_loss, train_acc = train(model, train_loader, optimizer)\n    val_loss, val_acc = validate(model, validation_loader)\n    elapsed_time = time.time() - start_time\n    \n    # save the weight when the loss gets smaller than last one\n    if val_loss < best_val_loss:\n        save_model(model, tc.device)\n        print(\"WEIGHTS-ARE-SAVED\")\n        best_val_loss = val_loss\n        \n    print(f\"Epoch{epoch+1}/{tc.epoch_count}, loss:{train_loss:.5f}, acc:{train_acc:.5f}, val_loss:{val_loss:.5f}, val_acc:{val_acc:.5f}, elapsed_time:{elapsed_time:.5f}\")\n    # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n    item = np.array([epoch+1, train_loss, train_acc, val_loss, val_acc, elapsed_time])\n    # store history for matplotlib visualization\n    history = np.vstack((history, item))\n\n    # For tensorboard\n    tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_loss, epoch+1)\n    tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count), train_acc, epoch+1)\n    tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now do the inference and get submission.csv file\ntest_dataset = KenyanFood13Testset()\ntest_loader = DataLoader(test_dataset, batch_size = tc.batch_size, shuffle = False)\n\n# prediction\npredicted_result = predict(model, test_loader).tolist()\n#print(\"predicted_result len:{}\".format(len(predicted_result)))\nclassifications = []\n\n#change predicted index to class_names\nfor index in range(len(predicted_result)):\n    predicted_result_class = test_dataset.base_dataset.index2class(int(predicted_result[index]))\n    classifications.append(predicted_result_class)\n\n# put it to csv file\nclasses = pd.DataFrame(classifications, columns = [\"class\"])\nresult = test_dataset.test_labels.join(classes)\nresult.to_csv(tc.submission_csv, index=False, header = True)","metadata":{"id":"WqFnOpy1RWSn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"f7bNvOcZRWSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(model, optimizer, train_loader, validation_loader, history):\n    \"\"\"works like tensorflow fits. epoch_count, optimizer and criterion(e.g., CrossEntorpyLoss) from training config\"\"\"\n    \n    #from tqdm.notebook import tqdm\n    \n    base_epochs = len(history)\n    \n    for epoch in range(base_epochs, tc.epoch_count + base_epochs):\n        train_loss = 0\n        train_acc = 0\n        val_loss = 0\n        val_acc = 0\n        best_valid_loss = np.Inf\n        \n        # training Phase\n        model.train()\n        count = 0\n        \n        t_begin = time.time()\n        for inputs, labels in train_loader:\n            \n            count += len(labels)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # initialize the gradients\n            optimizer.zero_grad()\n            # predict\n            outputs = model(inputs)\n            # calculate loss\n            loss = tc.criterion(outputs, labels)\n            train_loss += loss.item()\n            \n            #calucalte the graidents\n            loss.backward()\n            #update parameter\n            optimizer.step()\n            \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            train_acc  += (predicted == labels).sum().item()\n            \n            #calculate the average loss and accuracy in the batch\n            avg_train_loss = train_loss / count\n            avg_train_acc = train_acc / count\n            \n        #evaluation phase\n        model.eval()\n        count = 0\n        \n        for inputs, labels in validation_loader:\n            count += len(labels)\n            inputs = inputs.to(tc.device)\n            labels = labels.to(tc.device)\n            \n            # calculate the prediction\n            outputs = model(inputs)\n            \n            # calculate the loss\n            loss = tc.criterion(outputs, labels)\n            val_loss += loss.item()\n            \n            if val_loss < best_valid_loss:\n                torch.save(model.state_dict(), \"best_model.pt\")\n                print(\"WEIGHTS-ARE-SAVED\")\n                best_valid_loss = val_loss\n                \n            # get highest predicted value\n            predicted = torch.max(outputs, axis=1)[1]\n            # calculate number of correct answer\n            val_acc += (predicted == labels).sum().item()\n            \n            #calculate the average loss and accuracy in the batch\n            avg_val_loss = val_loss / count\n            avg_val_acc = val_acc / count\n            \n            \n        elapsed_time = time.time() - t_begin\n        # print out\n        print(\"Epoch{}/{},  loss:{:.5f}, acc:{:.5f}, val_loss:{:.5f}, val_acc:{:.5f}, elapsed_time:{:.5f}\".format(\n            epoch+1,tc.epoch_count+base_epochs, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time\n        ))\n        # store 0:epoch, 1:avg_train_loss, 2: avg_train_acc, 3: avg_val_loss, 4: avg_val_acc, 5: elapsed_time\n        item = np.array([epoch+1, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, elapsed_time])\n        history = np.vstack((history, item))\n        \n        # For tensorboard\n        tc.tb_writer.add_scalar('Loss/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_loss, epoch+1)\n        tc.tb_writer.add_scalar('Accuracy/Train within {}/{}'.format(epoch+1,tc.epoch_count+base_epochs), avg_train_acc, epoch+1)\n        tc.tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch+1)\n\n        \n    return history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size = tc.batch_size, shuffle = True)\nvalidation_loader = DataLoader(validation_dataset, batch_size = tc.batch_size, shuffle = False)\nhistory = np.zeros((0,6)) # history to record 0: num of iteration, 1:training_loss, 2:trainin_acc, 3:val_loss, 4:val_acc 5:elapsed time\noptimizer = SGD(model.parameters(), lr=tc.lr, momentum = 0.9)\nhistory = fit(model, optimizer, train_loader, validation_loader, history)","metadata":{"id":"9vg8rIMjRWSm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n\n**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\nFor example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars).","metadata":{"id":"QRbjuol1RWSo"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"id":"NHuqwjtrRWSo"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{"id":"zP0rAnxrRWSo"}}]}